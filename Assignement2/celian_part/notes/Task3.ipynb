{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9905936d",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## Task 3:\n",
    "*Replace the learned attention mechanism with an attention mechanism based on the Cosine similarity between node vectors.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288b94f",
   "metadata": {},
   "source": [
    "#### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d32526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5783b",
   "metadata": {},
   "source": [
    "#### Data Aggregation from Multiple Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36208d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing .nodes and .edges files\n",
    "DATASET_PATH = \"dataset\"\n",
    "\n",
    "# Get all scene IDs (each scene = a file pair: .nodes + .edges)\n",
    "scene_ids = sorted(set(f.split(\".\")[0] for f in os.listdir(DATASET_PATH)))\n",
    "\n",
    "# Lists to store combined node and edge data\n",
    "all_nodes = []\n",
    "all_edges = []\n",
    "offset = 0 # Offset to make node IDs unique across all scenes\n",
    "\n",
    "for scene_id in scene_ids:\n",
    "    nodes_path = os.path.join(DATASET_PATH, f\"{scene_id}.nodes\")\n",
    "    edges_path = os.path.join(DATASET_PATH, f\"{scene_id}.edges\")\n",
    "    \n",
    "    if not os.path.exists(nodes_path) or not os.path.exists(edges_path):\n",
    "        continue # Skip if files are missing\n",
    "    \n",
    "    # Load node attributes\n",
    "    nodes_df = pd.read_csv(\n",
    "        nodes_path, header=None,\n",
    "        names=[\"node_id\", \"current_x\", \"current_y\", \"prev_x\", \"prev_y\", \"future_x\", \"future_y\"],\n",
    "        na_values=[\"_\", \"NA\", \"NaN\", \"nan\"]\n",
    "    )\n",
    "    \n",
    "    # Remove rows with missing position data (except for future positions)\n",
    "    nodes_df = nodes_df.dropna(subset=[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"])\n",
    "    nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]] = nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].astype(float)\n",
    "    \n",
    "    # Reindex local node IDs into global unique IDs\n",
    "    local_to_global = {nid: i + offset for i, nid in enumerate(nodes_df[\"node_id\"])}\n",
    "    nodes_df[\"global_id\"] = nodes_df[\"node_id\"].map(local_to_global)\n",
    "    \n",
    "    # Load and remap edge list to global IDs\n",
    "    edges_df = pd.read_csv(edges_path, header=None, names=[\"target\", \"source\"])\n",
    "    edges_df[\"source\"] = edges_df[\"source\"].map(local_to_global)\n",
    "    edges_df[\"target\"] = edges_df[\"target\"].map(local_to_global)\n",
    "    \n",
    "    # Duplicate edges in the opposite direction to make graph undirected\n",
    "    reversed_edges = edges_df.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "    full_edges = pd.concat([edges_df, reversed_edges])\n",
    "    \n",
    "    # Store the processed scene data\n",
    "    all_nodes.append(nodes_df)\n",
    "    all_edges.append(full_edges)\n",
    "\n",
    "    # Update offset for next scene\n",
    "    offset += len(nodes_df)\n",
    "    \n",
    "# Concatenate all scenes into single global DataFrames\n",
    "nodes_all = pd.concat(all_nodes).sort_values(\"global_id\")\n",
    "edges_all = pd.concat(all_edges).dropna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f27c30",
   "metadata": {},
   "source": [
    "#### Tensor Creation for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract node input features as float tensor (shape: [num_nodes, 4])\n",
    "node_features = tf.convert_to_tensor(\n",
    "    nodes_all[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Extract labels (future positions), replacing NaN with 0.0 to avoid tensor errors\n",
    "labels = tf.convert_to_tensor(\n",
    "    nodes_all[[\"future_x\", \"future_y\"]].fillna(0.0).to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Create a mask to mark which nodes have valid future positions (used for loss filtering)\n",
    "mask = tf.convert_to_tensor(\n",
    "    ~nodes_all[[\"future_x\", \"future_y\"]].isna().any(axis=1),\n",
    "    dtype=tf.bool\n",
    ")\n",
    "\n",
    "# Create edge list as tensor (shape: [num_edges, 2]), each row: [target, source]\n",
    "edges = tf.convert_to_tensor(\n",
    "    edges_all[[\"target\", \"source\"]].to_numpy(),\n",
    "    dtype=tf.int64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cd81e",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle all node indices and split 80/20 for training and testing\n",
    "split_ratio = 0.8\n",
    "random_indices = np.random.permutation(len(nodes_all)) # Randomly permuted indices\n",
    "split_idx = int(split_ratio * len(random_indices))\n",
    "train_indices = random_indices[: split_idx]\n",
    "test_indices = random_indices[split_idx :]\n",
    "\n",
    "# Select node features and labels using the indices (converted to NumPy first)\n",
    "train_node_features = node_features.numpy()[train_indices]\n",
    "test_node_features = node_features.numpy()[test_indices]\n",
    "\n",
    "train_labels = labels.numpy()[train_indices]\n",
    "test_labels = labels.numpy()[test_indices]\n",
    "\n",
    "train_mask = mask.numpy()[train_indices]\n",
    "test_mask = mask.numpy()[test_indices]\n",
    "\n",
    "# Convert everything back to TensorFlow tensors\n",
    "train_node_features = tf.convert_to_tensor(train_node_features, dtype=tf.float32)\n",
    "test_node_features = tf.convert_to_tensor(test_node_features, dtype=tf.float32)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "\n",
    "train_mask = tf.convert_to_tensor(train_mask, dtype=tf.bool)\n",
    "test_mask = tf.convert_to_tensor(test_mask, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e505f",
   "metadata": {},
   "source": [
    "*Display*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363560cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training features:\", train_node_features.shape)\n",
    "print(\"Shape of training labels:\", train_labels.shape)\n",
    "print(\"Shape of training masks:\", train_mask.shape)\n",
    "\n",
    "print(\"Shape of test features:\", test_node_features.shape)\n",
    "print(\"Shape of test labels:\", test_labels.shape)\n",
    "print(\"Shape of test masks:\", test_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7d423",
   "metadata": {},
   "source": [
    "#### Cosine-Based Graph Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # input_shape[0] is (num_nodes, feature_dim)\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        # 1) Linear transformation of node features\n",
    "        h = tf.matmul(node_states, self.kernel)  # (N, units)\n",
    "        \n",
    "        # 2) Gather transformed features of target and source nodes for each edge\n",
    "        h_target = tf.gather(h, edges[:, 0])  # (E, units)\n",
    "        h_source = tf.gather(h, edges[:, 1])  # (E, units)\n",
    "        \n",
    "        # 3) Compute cosine similarity as attention scores (no learned weights)\n",
    "        dot_prod = tf.reduce_sum(h_target * h_source, axis=1)  # (E,)\n",
    "        norm_target = tf.norm(h_target, axis=1)\n",
    "        norm_source = tf.norm(h_source, axis=1)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        cosine_sim = dot_prod / (norm_target * norm_source + epsilon)  # (E,)\n",
    "        \n",
    "        # 4) Normalize scores across incoming edges to each node\n",
    "        scores_exp = tf.exp(cosine_sim)\n",
    "        denom = tf.math.unsorted_segment_sum(\n",
    "            data=scores_exp,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        # broadcast denom back to each edge\n",
    "        denom_per_edge = tf.gather(denom, edges[:, 0])\n",
    "        alpha = scores_exp / (denom_per_edge + epsilon)\n",
    "        \n",
    "        # 5) Weighted aggregation of neighbor states\n",
    "        neigh = tf.gather(h, edges[:, 1])             # (E, units)\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=neigh * alpha[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84d39b",
   "metadata": {},
   "source": [
    ">   - This version replaces the learned attention mechanism (trainable weights) with a **fixed cosine similarity** score between connected nodes.\n",
    ">   - **Cosine similarity** measures alignment between node embeddings, ranging from -1 to 1, and reflects **structural similarity** in the latent space.\n",
    ">   - The attention weights (`α`) are still normalized per target node using a softmax-like approach for proper aggregation.\n",
    ">   - This approach reduces the model's **learnable complexity** but can lead to **more stable and interpretable behavior**.\n",
    ">   - Ideal for evaluating whether attention learning provides significant gains over geometric or similarity-based heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f0afd",
   "metadata": {},
   "source": [
    "#### Define Multi-Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        # create one GraphAttention per head\n",
    "        self.attention_heads = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        head_outputs = [head([node_states, edges]) for head in self.attention_heads]\n",
    "        if self.merge_type == \"concat\":\n",
    "            h = tf.concat(head_outputs, axis=-1)  # (N, units * num_heads)\n",
    "        else:\n",
    "            h = tf.reduce_mean(tf.stack(head_outputs, axis=-1), axis=-1)  # (N, units)\n",
    "        return tf.nn.relu(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d40777",
   "metadata": {},
   "source": [
    "#### Define Graph Attention Network (GAT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320666fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features,\n",
    "        edges,\n",
    "        hidden_units=32,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        output_dim=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        # Store fixed graph structure\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "\n",
    "        # Deeper transformation of node features using two fully connected layers\n",
    "        self.preprocess = keras.Sequential([\n",
    "            layers.Dense(hidden_units * num_heads, activation=\"relu\", name=\"embedding_fc1\"),\n",
    "            layers.Dense(hidden_units * num_heads, name=\"embedding_fc2\"),\n",
    "        ], name=\"node_embedding\")\n",
    "\n",
    "        # Stack multiple multi-head GAT layers\n",
    "        self.gat_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads, merge_type=\"concat\")\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Output layer: regresses to (x, y)\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, edges = inputs\n",
    "\n",
    "        # Initial feature transformation\n",
    "        x = self.preprocess(x)\n",
    "\n",
    "        # Apply GAT layers with residual connections\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat([x, edges]) + x\n",
    "\n",
    "        # Final output layer for (x, y) coordinates\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, y_true = data\n",
    "\n",
    "        # Custom training loop with batch sampling\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self([self.node_features, self.edges])\n",
    "            y_pred_batch = tf.gather(y_pred, indices)\n",
    "            loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        # Track metrics\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, y_true = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        y_pred_batch = tf.gather(y_pred, indices)\n",
    "        loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        return tf.gather(y_pred, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e8053",
   "metadata": {},
   "source": [
    "#### Define Training Configuration and Hyperparameter Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Hyperparameters -----------\n",
    "HIDDEN_UNITS = 32       # size of each GAT attention head\n",
    "NUM_LAYERS = 2          # number of stacked GAT layers\n",
    "OUTPUT_DIM = 2          # we predict 2 values: (future_x, future_y)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1  # 10% of train data used for validation\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 30\n",
    "\n",
    "HEAD_OPTIONS = [2, 4, 8]\n",
    "\n",
    "# ----------- Loss and optimizer -----------\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "# ----------- Early stopping to prevent overfitting -----------\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\",          # monitor validation MAE\n",
    "    min_delta=1e-4,             # minimal change to be considered improvement\n",
    "    patience=PATIENCE,          # stop if no improvement over X epochs\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60dba3",
   "metadata": {},
   "source": [
    "#### Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}\n",
    "mae_results = {}\n",
    "predictions = {}\n",
    "\n",
    "for num_heads in HEAD_OPTIONS:\n",
    "    print(f\"\\n--- Training model with {num_heads} attention heads ---\")\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # Initialize a GAT model with the given number of attention heads\n",
    "    gat_model = GraphAttentionNetwork(\n",
    "        node_features=node_features,\n",
    "        edges=edges,\n",
    "        hidden_units=HIDDEN_UNITS,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        output_dim=OUTPUT_DIM\n",
    "    )\n",
    "\n",
    "    # Compile with MSE loss and MAE metric\n",
    "    gat_model.compile(\n",
    "        loss=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        metrics=[mae_metric]\n",
    "    )\n",
    "\n",
    "    # Train the model on node indices (mini-batch), full graph is fixed\n",
    "    history = gat_model.fit(\n",
    "        x=train_indices,\n",
    "        y=train_labels,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Evaluate performance on test set\n",
    "    loss, mae = gat_model.evaluate(\n",
    "        x=test_indices,\n",
    "        y=test_labels,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Store results for comparison\n",
    "    histories[num_heads] = history.history\n",
    "    mae_results[num_heads] = mae\n",
    "    predictions[num_heads] = gat_model.predict(x=test_indices)\n",
    "\n",
    "# Final predictions correspond to the last model trained (with highest num_heads)\n",
    "test_preds = gat_model.predict(x=test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf67f06",
   "metadata": {},
   "source": [
    "#### Validation MAE Curves — Attention Head Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE curves on the validation set (to compare convergence behaviour)plt.figure(figsize=(10, 6))\n",
    "plt.figure(figsize=(10, 6))\n",
    "for num_heads in HEAD_OPTIONS:\n",
    "    val_mae = histories[num_heads]['val_mae']\n",
    "    plt.plot(val_mae, label=f\"{num_heads} heads\")\n",
    "plt.title(\"Validation MAE per number of attention heads\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9e9ca",
   "metadata": {},
   "source": [
    "#### Test MAE Comparison by Attention Head Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0341126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart summarizing test MAE for each model (by number of attention heads)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.bar(\n",
    "    [str(h) + \" heads\" for h in mae_results.keys()],\n",
    "    [mae_results[h] for h in mae_results],\n",
    "    color=\"skyblue\"\n",
    ")\n",
    "plt.title(\"Test MAE per model\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb83b3",
   "metadata": {},
   "source": [
    "#### Prediction Samples from Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a916fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of a few predictions vs ground truth for the best model\n",
    "best_heads = min(mae_results, key=mae_results.get)\n",
    "best_preds = predictions[best_heads]\n",
    "\n",
    "print(f\"\\nBest model: {best_heads} attention heads (Test MAE = {mae_results[best_heads]:.4f})\")\n",
    "\n",
    "# Initialize the list for storing error data\n",
    "errors = []\n",
    "\n",
    "# Loop through the predictions and compute the error\n",
    "for i, (pred, true) in enumerate(zip(best_preds, test_labels)):\n",
    "    true_norm = tf.norm(true).numpy()  # L2 norm of ground truth\n",
    "    l2_error = tf.norm(pred - true).numpy()  # L2 error between prediction and ground truth\n",
    "    errors.append({\n",
    "        \"index\": i,\n",
    "        \"prediction\": pred,\n",
    "        \"ground_truth\": true,\n",
    "        \"l2_error\": l2_error\n",
    "    })\n",
    "\n",
    "# Sort the errors in ascending order of L2 error (smallest error first)\n",
    "errors_sorted = sorted(errors, key=lambda x: x[\"l2_error\"], reverse=False)\n",
    "\n",
    "# Display the top 5 examples with the smallest errors\n",
    "num_examples = 5\n",
    "for i, item in enumerate(errors_sorted[:num_examples]):\n",
    "    pred = item[\"prediction\"]\n",
    "    true = item[\"ground_truth\"]\n",
    "    l2_error = item[\"l2_error\"]\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"\\tPrediction   = ({pred[0]:.2f}, {pred[1]:.2f})\")\n",
    "    print(f\"\\tGround Truth = ({true[0]:.2f}, {true[1]:.2f})\")\n",
    "    print(f\"\\tL2 Error     = {l2_error:.2f} units\")\n",
    "    print(\"---\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
