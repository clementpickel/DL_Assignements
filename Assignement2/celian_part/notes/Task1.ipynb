{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb9030a",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## Task 1:\n",
    "*Adjust the tutorial implementation to perform the given prediction task and perform a suitable evaluation on a dedicated test set.*\n",
    "\n",
    "*For example, you can compute the Euclidean distance between the target point and the predicted point.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f7fdc",
   "metadata": {},
   "source": [
    "#### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c011f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7da2f",
   "metadata": {},
   "source": [
    ">   - `pandas` is used to read and manipulate CSV files (your node and edge data).\n",
    ">   - `numpy` is useful for numerical arrays and vectorized math operations.\n",
    ">   - `tensorflow` & `keras` will be used to build and train the GAT model.\n",
    ">   - `layers` gives access to Keras components like Dense layers, activation functions, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e095e",
   "metadata": {},
   "source": [
    "#### Data Aggregation from Multiple Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing .nodes and .edges files\n",
    "DATASET_PATH = \"dataset\"\n",
    "\n",
    "# Get all scene IDs (each scene = a file pair: .nodes + .edges)\n",
    "scene_ids = sorted(set(f.split(\".\")[0] for f in os.listdir(DATASET_PATH)))\n",
    "\n",
    "# Lists to store combined node and edge data\n",
    "all_nodes = []\n",
    "all_edges = []\n",
    "offset = 0 # Offset to make node IDs unique across all scenes\n",
    "\n",
    "for scene_id in scene_ids:\n",
    "    nodes_path = os.path.join(DATASET_PATH, f\"{scene_id}.nodes\")\n",
    "    edges_path = os.path.join(DATASET_PATH, f\"{scene_id}.edges\")\n",
    "    \n",
    "    if not os.path.exists(nodes_path) or not os.path.exists(edges_path):\n",
    "        continue # Skip if files are missing\n",
    "    \n",
    "    # Load node attributes\n",
    "    nodes_df = pd.read_csv(\n",
    "        nodes_path, header=None,\n",
    "        names=[\"node_id\", \"current_x\", \"current_y\", \"prev_x\", \"prev_y\", \"future_x\", \"future_y\"],\n",
    "        na_values=[\"_\", \"NA\", \"NaN\", \"nan\"]\n",
    "    )\n",
    "    \n",
    "    # Remove rows with missing position data (except for future positions)\n",
    "    nodes_df = nodes_df.dropna(subset=[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"])\n",
    "    nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]] = nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].astype(float)\n",
    "    \n",
    "    # Reindex local node IDs into global unique IDs\n",
    "    local_to_global = {nid: i + offset for i, nid in enumerate(nodes_df[\"node_id\"])}\n",
    "    nodes_df[\"global_id\"] = nodes_df[\"node_id\"].map(local_to_global)\n",
    "    \n",
    "    # Load and remap edge list to global IDs\n",
    "    edges_df = pd.read_csv(edges_path, header=None, names=[\"target\", \"source\"])\n",
    "    edges_df[\"source\"] = edges_df[\"source\"].map(local_to_global)\n",
    "    edges_df[\"target\"] = edges_df[\"target\"].map(local_to_global)\n",
    "    \n",
    "    # Duplicate edges in the opposite direction to make graph undirected\n",
    "    reversed_edges = edges_df.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "    full_edges = pd.concat([edges_df, reversed_edges])\n",
    "    \n",
    "    # Store the processed scene data\n",
    "    all_nodes.append(nodes_df)\n",
    "    all_edges.append(full_edges)\n",
    "\n",
    "    # Update offset for next scene\n",
    "    offset += len(nodes_df)\n",
    "    \n",
    "# Concatenate all scenes into single global DataFrames\n",
    "nodes_all = pd.concat(all_nodes).sort_values(\"global_id\")\n",
    "edges_all = pd.concat(all_edges).dropna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af160a6",
   "metadata": {},
   "source": [
    ">   - This block **loads and aggregates multiple scenes** into one global graph structure.  \n",
    ">   - Node IDs are **reindexed globally** to avoid collisions between scenes.  \n",
    ">   - Missing `current` or `previous` positions are filtered out.  \n",
    "        `future_x/y` is kept (even if missing) because it’s only needed at **prediction time**.  \n",
    ">   - Graph is made **undirected** by manually adding reverse edges.\n",
    ">   - Final output:  \n",
    ">       - `nodes_all`: contains all node features, cleaned and reindexed.  \n",
    ">       - `edges_all`: contains all edges across all scenes, with global node IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fcf35a",
   "metadata": {},
   "source": [
    "#### Tensor Creation for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423faee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract node input features as float tensor (shape: [num_nodes, 4])\n",
    "node_features = tf.convert_to_tensor(\n",
    "    nodes_all[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Extract labels (future positions), replacing NaN with 0.0 to avoid tensor errors\n",
    "labels = tf.convert_to_tensor(\n",
    "    nodes_all[[\"future_x\", \"future_y\"]].fillna(0.0).to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Create a mask to mark which nodes have valid future positions (used for loss filtering)\n",
    "mask = tf.convert_to_tensor(\n",
    "    ~nodes_all[[\"future_x\", \"future_y\"]].isna().any(axis=1),\n",
    "    dtype=tf.bool\n",
    ")\n",
    "\n",
    "# Create edge list as tensor (shape: [num_edges, 2]), each row: [target, source]\n",
    "edges = tf.convert_to_tensor(\n",
    "    edges_all[[\"target\", \"source\"]].to_numpy(),\n",
    "    dtype=tf.int64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123adfa",
   "metadata": {},
   "source": [
    ">   - `node_features`: 4D input vector for each node (current + previous positions).\n",
    ">   - `labels`: target (x, y) positions, with NaNs replaced by `0.0` to keep tensor shape consistent.\n",
    ">   - `mask`: boolean mask **to filter valid training samples** (nodes with known future positions).\n",
    ">   - `edges`: edge list formatted for a GNN — each edge connects two nodes using global IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dae595",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle all node indices and split 80/20 for training and testing\n",
    "split_ratio = 0.8\n",
    "random_indices = np.random.permutation(len(nodes_all)) # Randomly permuted indices\n",
    "split_idx = int(split_ratio * len(random_indices))\n",
    "train_indices = random_indices[: split_idx]\n",
    "test_indices = random_indices[split_idx :]\n",
    "\n",
    "# Select node features and labels using the indices (converted to NumPy first)\n",
    "train_node_features = node_features.numpy()[train_indices]\n",
    "test_node_features = node_features.numpy()[test_indices]\n",
    "\n",
    "train_labels = labels.numpy()[train_indices]\n",
    "test_labels = labels.numpy()[test_indices]\n",
    "\n",
    "train_mask = mask.numpy()[train_indices]\n",
    "test_mask = mask.numpy()[test_indices]\n",
    "\n",
    "# Convert everything back to TensorFlow tensors\n",
    "train_node_features = tf.convert_to_tensor(train_node_features, dtype=tf.float32)\n",
    "test_node_features = tf.convert_to_tensor(test_node_features, dtype=tf.float32)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "\n",
    "train_mask = tf.convert_to_tensor(train_mask, dtype=tf.bool)\n",
    "test_mask = tf.convert_to_tensor(test_mask, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ac1cf",
   "metadata": {},
   "source": [
    ">   - **50/50 train-test split** is done randomly over all nodes, not per scene.\n",
    ">   - Data is split **at the node level**: features, labels, and valid masks are all subset accordingly.\n",
    ">   - Tensors are briefly converted to NumPy for indexing, then **converted back to TensorFlow tensors**.\n",
    ">   - `train_mask` and `test_mask` will be used to compute the loss **only for nodes with valid labels**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5baff1",
   "metadata": {},
   "source": [
    "*Display*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training features:\", train_node_features.shape)\n",
    "print(\"Shape of training labels:\", train_labels.shape)\n",
    "print(\"Shape of training masks:\", train_mask.shape)\n",
    "\n",
    "print(\"Shape of test features:\", test_node_features.shape)\n",
    "print(\"Shape of test labels:\", test_labels.shape)\n",
    "print(\"Shape of test masks:\", test_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36c68e",
   "metadata": {},
   "source": [
    "#### Graph Attention Layer Definition (GAT Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Learnable linear projection for input features\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        # Attention kernel for computing attention scores [h_i || h_j] -> score\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        # Step 1: Linear transformation of node features\n",
    "        h = tf.matmul(node_states, self.kernel)  # (N, units)\n",
    "\n",
    "        # Step 2: Compute attention scores for each edge [h_i || h_j]\n",
    "        edge_states = tf.gather(h, edges)   # (E, 2, units)\n",
    "        edge_states = tf.reshape(edge_states, (tf.shape(edges)[0], 2 * self.units))\n",
    "        scores = tf.nn.leaky_relu(tf.matmul(edge_states, self.kernel_attention))    # (E,1)\n",
    "        scores = tf.squeeze(scores, -1)     # (E,)\n",
    "        \n",
    "        # Step 3: Normalize attention scores (softmax-like) per target node\n",
    "        scores_exp = tf.exp(tf.clip_by_value(scores, -2, 2))    # for stability\n",
    "        denom = tf.math.unsorted_segment_sum(\n",
    "            data=scores_exp,\n",
    "            segment_ids=edges[:, 0],    # group by target node\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        # broadcast denom back to each edge\n",
    "        denom_per_edge = tf.gather(denom, edges[:, 0])\n",
    "        alpha = scores_exp / (denom_per_edge + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Step 4: Weighted sum of neighbor features using attention scores\n",
    "        neigh = tf.gather(h, edges[:, 1])   # (E, units)\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=neigh * alpha[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],    # group by target node\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79348590",
   "metadata": {},
   "source": [
    ">   - This class defines a custom GAT layer (Graph Attention Layer).  \n",
    ">   - It follows the original GAT paper pipeline:  \n",
    ">       1. **Linear transformation** of node features: $h_i = W \\cdot x_i$  \n",
    ">       2. **Attention score** computation for each edge using concatenated features $[h_i||h_j]$  \n",
    ">       3. **Softmax-style normalization** across all neighbors of each target node  \n",
    ">       4. Weighted aggregation of neighbor states using attention weights $\\alpha_{ij}$  \n",
    ">   - `unsorted_segment_sum` is key: it groups and sums messages **per target node**.  \n",
    ">   - The graph is **directed in edges**, but the dataset was made undirected earlier (so info still flows both ways)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10357b9",
   "metadata": {},
   "source": [
    "#### Multi-Head Graph Attention Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27518948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        # Initialize multiple GAT heads\n",
    "        self.attention_heads = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        # Apply each attention head independently\n",
    "        head_outputs = [head([node_states, edges]) for head in self.attention_heads]\n",
    "\n",
    "        # Merge head outputs: either concatenate or average\n",
    "        if self.merge_type == \"concat\":\n",
    "            h = tf.concat(head_outputs, axis=-1)    # Shape: (N, units * num_heads)\n",
    "        else:\n",
    "            h = tf.reduce_mean(tf.stack(head_outputs, axis=-1), axis=-1)    # Shape: (N, units)\n",
    "\n",
    "        # Apply activation function\n",
    "        return tf.nn.relu(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3349766",
   "metadata": {},
   "source": [
    ">   - **Multi-head attention**: several `GraphAttention` layers (heads) run in parallel.  \n",
    ">   - Each head can focus on **different neighbors or patterns**, like different “views” of the graph.\n",
    ">   - Two ways to merge the heads:  \n",
    ">       - `\"concat\"`: concatenate all head outputs (like in the original GAT paper) $\\to$ richer representation.  \n",
    ">       - `\"average\"`: average across heads $\\to$ more compact, less parameters.\n",
    ">   - `tf.nn.relu` is applied after merging to introduce non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14a168",
   "metadata": {},
   "source": [
    "#### Graph Attention Network Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features,\n",
    "        edges,\n",
    "        hidden_units=32,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        output_dim=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        # Store fixed graph structure\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "\n",
    "        # Initial transformation of input features\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "\n",
    "        # Stack multiple multi-head GAT layers\n",
    "        self.gat_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads, merge_type=\"concat\")\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Output layer: regresses to (x, y)\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, edges = inputs\n",
    "\n",
    "        # Initial feature transformation\n",
    "        x = self.preprocess(x)\n",
    "\n",
    "        # Apply GAT layers with residual connections\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat([x, edges]) + x\n",
    "        # Final output layer for (x, y) coordinates\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, y_true = data\n",
    "\n",
    "        # Custom training loop with batch sampling\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self([self.node_features, self.edges])\n",
    "            y_pred_batch = tf.gather(y_pred, indices)   # Select batch samples\n",
    "            loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        # Track metrics\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, y_true = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        y_pred_batch = tf.gather(y_pred, indices)\n",
    "        loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        return tf.gather(y_pred, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b66c2",
   "metadata": {},
   "source": [
    ">   - `GraphAttentionNetwork` is a **custom Keras model** combining:\n",
    ">       - `Dense` layer for initial embedding.\n",
    ">       - A **stack of Multi-Head GAT layers** for message passing.\n",
    ">       - A final `Dense` layer for **regression to 2D output** (`(x, y)`).\n",
    ">   - The `call()` method does a forward pass through the model.\n",
    ">   - `train_step`, `test_step`, and `predict_step` are overridden to allow training/prediction using **node indices**, while the full graph stays in memory.\n",
    ">   - `+ x` in GAT loop $\\to$ **residual connection**, helps **gradient flow** and **stabilizes learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e925f0b",
   "metadata": {},
   "source": [
    "#### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Hyperparameters -----------\n",
    "HIDDEN_UNITS = 32       # size of each GAT attention head\n",
    "NUM_HEADS = 4           # number of parallel attention heads\n",
    "NUM_LAYERS = 2          # number of stacked GAT layers\n",
    "OUTPUT_DIM = 2          # we predict 2 values: (future_x, future_y)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1  # 10% of train data used for validation\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 10           # early stopping patience\n",
    "\n",
    "# ----------- Loss and optimizer -----------\n",
    "loss_fn = keras.losses.MeanSquaredError()                   # for regression\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")    # easier to interpret\n",
    "\n",
    "# ----------- Early stopping to prevent overfitting -----------\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\",          # monitor validation MAE\n",
    "    min_delta=1e-4,             # minimal change to be considered improvement\n",
    "    patience=PATIENCE,          # stop if no improvement over X epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ----------- Instantiate and compile the GAT model -----------\n",
    "gat_model = GraphAttentionNetwork(\n",
    "    node_features=node_features,\n",
    "    edges=edges,\n",
    "    hidden_units=HIDDEN_UNITS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    output_dim=OUTPUT_DIM\n",
    ")\n",
    "\n",
    "\n",
    "gat_model.compile(\n",
    "    loss=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[mae_metric]\n",
    ")\n",
    "\n",
    "# ----------- Train the model -----------\n",
    "gat_model.fit(\n",
    "    x=train_indices,\n",
    "    y=train_labels,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# ----------- Evaluate on test set -----------\n",
    "loss, mae = gat_model.evaluate(\n",
    "    x=test_indices,\n",
    "    y=test_labels,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2aaf59",
   "metadata": {},
   "source": [
    ">   - **Model hyperparameters** (`units`, `heads`, `layers`) define the GAT architecture capacity.\n",
    ">   - `MeanSquaredError` is the loss function used for regression (sensitive to outliers).\n",
    ">   - `MeanAbsoluteError` is used as a metric (more interpretable, in same unit as coordinates).\n",
    ">   - **EarlyStopping** helps prevent overfitting by monitoring `val_mae`.\n",
    ">   - The model is trained with:\n",
    ">       - full graph fixed (`node_features`, `edges`)\n",
    ">       - mini-batch sampling using node indices (`train_indices`)\n",
    ">       - training targets being future coordinates\n",
    ">   - The evaluation returns the final **test loss and MAE**, which reflect prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98105197",
   "metadata": {},
   "source": [
    "*Display*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41fd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTest MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9bfb1",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e82085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict future (x, y) positions for the test nodes\n",
    "# The model uses the fixed graph (features + edges) and returns predictions for selected indices\n",
    "test_preds = gat_model.predict(x=test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7defb3",
   "metadata": {},
   "source": [
    ">   - `gat_model.predict(x=test_indices)` returns the model's prediction for each node in `test_indices`.\n",
    ">   - The model uses the **fixed full graph structure** (defined during instantiation).\n",
    ">   - Only the **nodes corresponding to** `test_indices` are extracted from the prediction output using `predict_step`.\n",
    ">   - The output `test_preds` is a tensor of shape `(num_test_nodes, 2)` $\\to$ each row contains the predicted `(x, y)` future position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993c2a6",
   "metadata": {},
   "source": [
    "*Display*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 predictions with errors\n",
    "errors = []\n",
    "for i, (pred, true) in enumerate(zip(test_preds[:], test_labels[:])):\n",
    "    true_norm = tf.norm(true).numpy()\n",
    "    l2_error = tf.norm(pred - true).numpy()\n",
    "    errors.append({\n",
    "        \"index\": i,\n",
    "        \"prediction\": pred,\n",
    "        \"ground_truth\": true,\n",
    "        \"l2_error\": l2_error\n",
    "    })\n",
    "\n",
    "errors_sorted = sorted(errors, key=lambda x: x[\"l2_error\"], reverse=False)\n",
    "\n",
    "for i, item in enumerate(errors_sorted[:]):\n",
    "    pred = item[\"prediction\"]\n",
    "    true = item[\"ground_truth\"]\n",
    "    l2_error = item[\"l2_error\"]\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"\\tPrediction   = ({pred[0]:.2f}, {pred[1]:.2f})\")\n",
    "    print(f\"\\tGround Truth = ({true[0]:.2f}, {true[1]:.2f})\")\n",
    "    print(f\"\\tL2 Error     = {l2_error:.2f} units\")\n",
    "    print(\"---\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
