{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9905936d",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## Task 2:\n",
    "*Perform hyperparameter tuning of the number of attention heads and try a deeper embedding of the node features.*\n",
    "\n",
    "*For the attention heads, just evaluate 2 additional settings.*\n",
    "\n",
    "*For the embedding of the node features, instead of the linear transformation of the node states as suggested in the tutorial, try to add one fully connected layer with ReLU activation and one additional fully connected layer.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288b94f",
   "metadata": {},
   "source": [
    "#### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d32526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b8961",
   "metadata": {},
   "source": [
    ">   - The import of `matplotlib.pyplot` (`plt`) is new compared to Task 1.\n",
    ">   - It suggests that some form of **visualization (e.g., metric evolution or comparisons)** may be performed later â€” likely useful for **hyperparameter tuning analysis**.\n",
    ">   - All other imports are unchanged and reused from Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5783b",
   "metadata": {},
   "source": [
    "#### Data Aggregation from Multiple Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36208d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing .nodes and .edges files\n",
    "DATASET_PATH = \"dataset\"\n",
    "\n",
    "# Get all scene IDs (each scene = a file pair: .nodes + .edges)\n",
    "scene_ids = sorted(set(f.split(\".\")[0] for f in os.listdir(DATASET_PATH)))\n",
    "\n",
    "# Lists to store combined node and edge data\n",
    "all_nodes = []\n",
    "all_edges = []\n",
    "offset = 0 # Offset to make node IDs unique across all scenes\n",
    "\n",
    "for scene_id in scene_ids:\n",
    "    nodes_path = os.path.join(DATASET_PATH, f\"{scene_id}.nodes\")\n",
    "    edges_path = os.path.join(DATASET_PATH, f\"{scene_id}.edges\")\n",
    "    \n",
    "    if not os.path.exists(nodes_path) or not os.path.exists(edges_path):\n",
    "        continue # Skip if files are missing\n",
    "    \n",
    "    # Load node attributes\n",
    "    nodes_df = pd.read_csv(\n",
    "        nodes_path, header=None,\n",
    "        names=[\"node_id\", \"current_x\", \"current_y\", \"prev_x\", \"prev_y\", \"future_x\", \"future_y\"],\n",
    "        na_values=[\"_\", \"NA\", \"NaN\", \"nan\"]\n",
    "    )\n",
    "    \n",
    "    # Remove rows with missing position data (except for future positions)\n",
    "    nodes_df = nodes_df.dropna(subset=[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"])\n",
    "    nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]] = nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].astype(float)\n",
    "    \n",
    "    # Reindex local node IDs into global unique IDs\n",
    "    local_to_global = {nid: i + offset for i, nid in enumerate(nodes_df[\"node_id\"])}\n",
    "    nodes_df[\"global_id\"] = nodes_df[\"node_id\"].map(local_to_global)\n",
    "    \n",
    "    # Load and remap edge list to global IDs\n",
    "    edges_df = pd.read_csv(edges_path, header=None, names=[\"target\", \"source\"])\n",
    "    edges_df[\"source\"] = edges_df[\"source\"].map(local_to_global)\n",
    "    edges_df[\"target\"] = edges_df[\"target\"].map(local_to_global)\n",
    "    \n",
    "    # Duplicate edges in the opposite direction to make graph undirected\n",
    "    reversed_edges = edges_df.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "    full_edges = pd.concat([edges_df, reversed_edges])\n",
    "    \n",
    "    # Store the processed scene data\n",
    "    all_nodes.append(nodes_df)\n",
    "    all_edges.append(full_edges)\n",
    "\n",
    "    # Update offset for next scene\n",
    "    offset += len(nodes_df)\n",
    "    \n",
    "# Concatenate all scenes into single global DataFrames\n",
    "nodes_all = pd.concat(all_nodes).sort_values(\"global_id\")\n",
    "edges_all = pd.concat(all_edges).dropna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f27c30",
   "metadata": {},
   "source": [
    "#### Tensor Creation for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract node input features as float tensor (shape: [num_nodes, 4])\n",
    "node_features = tf.convert_to_tensor(\n",
    "    nodes_all[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Extract labels (future positions), replacing NaN with 0.0 to avoid tensor errors\n",
    "labels = tf.convert_to_tensor(\n",
    "    nodes_all[[\"future_x\", \"future_y\"]].fillna(0.0).to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Create a mask to mark which nodes have valid future positions (used for loss filtering)\n",
    "mask = tf.convert_to_tensor(\n",
    "    ~nodes_all[[\"future_x\", \"future_y\"]].isna().any(axis=1),\n",
    "    dtype=tf.bool\n",
    ")\n",
    "\n",
    "# Create edge list as tensor (shape: [num_edges, 2]), each row: [target, source]\n",
    "edges = tf.convert_to_tensor(\n",
    "    edges_all[[\"target\", \"source\"]].to_numpy(),\n",
    "    dtype=tf.int64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cd81e",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle all node indices and split 80/20 for training and testing\n",
    "split_ratio = 0.8\n",
    "random_indices = np.random.permutation(len(nodes_all)) # Randomly permuted indices\n",
    "split_idx = int(split_ratio * len(random_indices))\n",
    "train_indices = random_indices[: split_idx]\n",
    "test_indices = random_indices[split_idx :]\n",
    "\n",
    "# Select node features and labels using the indices (converted to NumPy first)\n",
    "train_node_features = node_features.numpy()[train_indices]\n",
    "test_node_features = node_features.numpy()[test_indices]\n",
    "\n",
    "train_labels = labels.numpy()[train_indices]\n",
    "test_labels = labels.numpy()[test_indices]\n",
    "\n",
    "train_mask = mask.numpy()[train_indices]\n",
    "test_mask = mask.numpy()[test_indices]\n",
    "\n",
    "# Convert everything back to TensorFlow tensors\n",
    "train_node_features = tf.convert_to_tensor(train_node_features, dtype=tf.float32)\n",
    "test_node_features = tf.convert_to_tensor(test_node_features, dtype=tf.float32)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "\n",
    "train_mask = tf.convert_to_tensor(train_mask, dtype=tf.bool)\n",
    "test_mask = tf.convert_to_tensor(test_mask, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e505f",
   "metadata": {},
   "source": [
    "*Display*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363560cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training features:\", train_node_features.shape)\n",
    "print(\"Shape of training labels:\", train_labels.shape)\n",
    "print(\"Shape of training masks:\", train_mask.shape)\n",
    "\n",
    "print(\"Shape of test features:\", test_node_features.shape)\n",
    "print(\"Shape of test labels:\", test_labels.shape)\n",
    "print(\"Shape of test masks:\", test_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7d423",
   "metadata": {},
   "source": [
    "#### Graph Attention Layer Definition (GAT Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # input_shape[0] is (num_nodes, feature_dim)\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        # attention kernel takes concatenated pair [h_i || h_j]\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        # 1) Linear transform\n",
    "        h = tf.matmul(node_states, self.kernel)  # (N, units)\n",
    "\n",
    "        # 2) Compute attention scores for each edge\n",
    "        #    gather [h_target, h_source]\n",
    "        edge_states = tf.gather(h, edges)            # (E, 2, units)\n",
    "        edge_states = tf.reshape(\n",
    "            edge_states, (tf.shape(edges)[0], 2 * self.units)\n",
    "        )\n",
    "        scores = tf.nn.leaky_relu(\n",
    "            tf.matmul(edge_states, self.kernel_attention)\n",
    "        )  # (E,1)\n",
    "        scores = tf.squeeze(scores, -1)               # (E,)\n",
    "        \n",
    "        # 3) Normalize per target node\n",
    "        scores_exp = tf.exp(tf.clip_by_value(scores, -2, 2))\n",
    "        denom = tf.math.unsorted_segment_sum(\n",
    "            data=scores_exp,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        # broadcast denom back to each edge\n",
    "        denom_per_edge = tf.gather(denom, edges[:, 0])\n",
    "        alpha = scores_exp / (denom_per_edge + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # 4) Weighted aggregation of neighbor states\n",
    "        neigh = tf.gather(h, edges[:, 1])             # (E, units)\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=neigh * alpha[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f0afd",
   "metadata": {},
   "source": [
    "#### Define Multi-Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        # create one GraphAttention per head\n",
    "        self.attention_heads = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        head_outputs = [head([node_states, edges]) for head in self.attention_heads]\n",
    "        if self.merge_type == \"concat\":\n",
    "            h = tf.concat(head_outputs, axis=-1)  # (N, units * num_heads)\n",
    "        else:\n",
    "            h = tf.reduce_mean(tf.stack(head_outputs, axis=-1), axis=-1)  # (N, units)\n",
    "        return tf.nn.relu(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d40777",
   "metadata": {},
   "source": [
    "#### Define Graph Attention Network (GAT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320666fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features,\n",
    "        edges,\n",
    "        hidden_units=32,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        output_dim=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        # Store fixed graph structure\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "\n",
    "        # Deeper transformation of node features using two fully connected layers\n",
    "        self.preprocess = keras.Sequential([\n",
    "            layers.Dense(hidden_units * num_heads, activation=\"relu\", name=\"embedding_fc1\"),\n",
    "            layers.Dense(hidden_units * num_heads, name=\"embedding_fc2\"),\n",
    "        ], name=\"node_embedding\")\n",
    "\n",
    "        # Stack multiple multi-head GAT layers\n",
    "        self.gat_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads, merge_type=\"concat\")\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Output layer: regresses to (x, y)\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, edges = inputs\n",
    "\n",
    "        # Initial feature transformation\n",
    "        x = self.preprocess(x)\n",
    "\n",
    "        # Apply GAT layers with residual connections\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat([x, edges]) + x\n",
    "        \n",
    "        # Final output layer for (x, y) coordinates\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, y_true = data\n",
    "\n",
    "        # Custom training loop with batch sampling\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self([self.node_features, self.edges])\n",
    "            y_pred_batch = tf.gather(y_pred, indices)   # Select batch samples\n",
    "            loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        # Track metrics\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, y_true = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        y_pred_batch = tf.gather(y_pred, indices)\n",
    "        loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        return tf.gather(y_pred, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38770ee",
   "metadata": {},
   "source": [
    ">   - **Purpose**: Improves the modelâ€™s capacity to capture nonlinear feature interactions prior to message passing.\n",
    ">   - **Details**:\n",
    ">       - The first `Dense` layer applies a ReLU-activated transformation to introduce non-linearity and richer representations.\n",
    ">       - The second `Dense` layer serves to consolidate and project the features to the same dimensionality expected by the GAT layers.\n",
    ">       - This structure increases model expressiveness and supports better downstream attention computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e8053",
   "metadata": {},
   "source": [
    "#### Define Training Configuration and Hyperparameter Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Hyperparameters -----------\n",
    "HIDDEN_UNITS = 32       # size of each GAT attention head\n",
    "NUM_LAYERS = 2          # number of stacked GAT layers\n",
    "OUTPUT_DIM = 2          # we predict 2 values: (future_x, future_y)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1  # 10% of train data used for validation\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 30\n",
    "\n",
    "HEAD_OPTIONS = [2, 4, 8]\n",
    "\n",
    "# ----------- Loss and optimizer -----------\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "# ----------- Early stopping to prevent overfitting -----------\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\",          # monitor validation MAE\n",
    "    min_delta=1e-4,             # minimal change to be considered improvement\n",
    "    patience=PATIENCE,          # stop if no improvement over X epochs\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bbb85",
   "metadata": {},
   "source": [
    ">   - `HEAD_OPTIONS = [2, 4, 8]` defines the hyperparameter tuning space for attention heads.\n",
    ">   - The node feature embedding uses **two Dense layers** instead of a single linear projection.\n",
    ">   - `HIDDEN_UNITS`, `NUM_LAYERS`, and `OUTPUT_DIM` define GAT model depth and output shape.\n",
    ">   - `LEARNING_RATE = 1e-4` is lower than usual to support deeper, more expressive architectures.\n",
    ">   - `EarlyStopping` is more tolerant (`patience=30`) to accommodate slower convergence.\n",
    ">   - Loss: `MeanSquaredError`, suitable for regression; Metric: `MeanAbsoluteError`, interpretable.\n",
    ">   - This section **does not compile or train** models yet â€” it only sets up the config space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60dba3",
   "metadata": {},
   "source": [
    "#### Train and Compare GAT Models with Varying Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}\n",
    "mae_results = {}\n",
    "predictions = {}\n",
    "\n",
    "for num_heads in HEAD_OPTIONS:\n",
    "    print(f\"\\n--- Training model with {num_heads} attention heads ---\")\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # Initialize a GAT model with the given number of attention heads\n",
    "    gat_model = GraphAttentionNetwork(\n",
    "        node_features=node_features,\n",
    "        edges=edges,\n",
    "        hidden_units=HIDDEN_UNITS,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        output_dim=OUTPUT_DIM\n",
    "    )\n",
    "\n",
    "    # Compile with MSE loss and MAE metric\n",
    "    gat_model.compile(\n",
    "        loss=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        metrics=[mae_metric]\n",
    "    )\n",
    "\n",
    "    # Train the model on node indices (mini-batch), full graph is fixed\n",
    "    history = gat_model.fit(\n",
    "        x=train_indices,\n",
    "        y=train_labels,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Evaluate performance on test set\n",
    "    loss, mae = gat_model.evaluate(\n",
    "        x=test_indices,\n",
    "        y=test_labels,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Store results for comparison\n",
    "    histories[num_heads] = history.history\n",
    "    mae_results[num_heads] = mae\n",
    "    predictions[num_heads] = gat_model.predict(x=test_indices)\n",
    "\n",
    "# Final predictions correspond to the last model trained (with highest num_heads)\n",
    "test_preds = gat_model.predict(x=test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf67f06",
   "metadata": {},
   "source": [
    "#### Validation MAE Curves â€” Attention Head Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE curves on the validation set (to compare convergence behaviour)plt.figure(figsize=(10, 6))\n",
    "for num_heads in HEAD_OPTIONS:\n",
    "    val_mae = histories[num_heads]['val_mae']\n",
    "    plt.plot(val_mae, label=f\"{num_heads} heads\")\n",
    "plt.title(\"Validation MAE per number of attention heads\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcdced4",
   "metadata": {},
   "source": [
    ">   - This plot compares **how each model configuration (based on number of attention heads)** performs during training, using the **validation MAE** as the main reference.\n",
    ">   - **MAE (Mean Absolute Error)** measures the **average absolute difference between predicted and true values**. In this case, it reflects the **average distance error (in units) between the predicted and actual future positions** of nodes (i.e., agents).\n",
    ">   - A lower MAE means more **accurate trajectory prediction**, while a high MAE indicates the model is either underfitting or struggling with generalization.\n",
    ">   - The plot helps assess whether increasing the number of attention heads improves predictive performance or leads to overfitting.\n",
    ">   - A **faster MAE decrease and lower final MAE** are signs of better convergence and more effective learning dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9e9ca",
   "metadata": {},
   "source": [
    "#### Test MAE Comparison by Attention Head Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0341126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart summarizing test MAE for each model (by number of attention heads)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.bar(\n",
    "    [str(h) + \" heads\" for h in mae_results.keys()],\n",
    "    [mae_results[h] for h in mae_results],\n",
    "    color=\"skyblue\"\n",
    ")\n",
    "plt.title(\"Test MAE per model\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d0634",
   "metadata": {},
   "source": [
    ">   - This bar chart summarizes the **final model performance on the test set**, after training with different numbers of attention heads.\n",
    ">   - It displays the **Mean Absolute Error (MAE)** on **unseen test nodes**, providing an estimate of **generalization capability**.\n",
    ">   - Each bar corresponds to a trained GAT model using a specific number of attention heads (e.g., 2, 4, or 8).\n",
    ">   - Lower MAE values indicate **more accurate future position predictions**.\n",
    ">   - This comparison is useful to:\n",
    ">       - Validate the benefit (or not) of using more attention heads.\n",
    ">       - Detect overfitting if higher-capacity models (more heads) donâ€™t improve or worsen test MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd39df",
   "metadata": {},
   "source": [
    "*Display*:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb83b3",
   "metadata": {},
   "source": [
    "#### Prediction Samples from Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a916fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of a few predictions vs ground truth for the best model\n",
    "best_heads = min(mae_results, key=mae_results.get)\n",
    "best_preds = predictions[best_heads]\n",
    "\n",
    "print(f\"\\nBest model: {best_heads} attention heads (Test MAE = {mae_results[best_heads]:.4f})\")\n",
    "\n",
    "# Initialize the list for storing error data\n",
    "errors = []\n",
    "\n",
    "# Loop through the predictions and compute the error\n",
    "for i, (pred, true) in enumerate(zip(best_preds, test_labels)):\n",
    "    true_norm = tf.norm(true).numpy()  # L2 norm of ground truth\n",
    "    l2_error = tf.norm(pred - true).numpy()  # L2 error between prediction and ground truth\n",
    "    errors.append({\n",
    "        \"index\": i,\n",
    "        \"prediction\": pred,\n",
    "        \"ground_truth\": true,\n",
    "        \"l2_error\": l2_error\n",
    "    })\n",
    "\n",
    "# Sort the errors in ascending order of L2 error (smallest error first)\n",
    "errors_sorted = sorted(errors, key=lambda x: x[\"l2_error\"], reverse=False)\n",
    "\n",
    "# Display the top 5 examples with the smallest errors\n",
    "num_examples = 5\n",
    "for i, item in enumerate(errors_sorted[:num_examples]):\n",
    "    pred = item[\"prediction\"]\n",
    "    true = item[\"ground_truth\"]\n",
    "    l2_error = item[\"l2_error\"]\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"\\tPrediction   = ({pred[0]:.2f}, {pred[1]:.2f})\")\n",
    "    print(f\"\\tGround Truth = ({true[0]:.2f}, {true[1]:.2f})\")\n",
    "    print(f\"\\tL2 Error     = {l2_error:.2f} units\")\n",
    "    print(\"---\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
