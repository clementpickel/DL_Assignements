{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9905936d",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## Task 2:\n",
    "*Perform hyperparameter tuning of the number of attention heads and try a deeper embedding of the node features.*\n",
    "\n",
    "*For the attention heads, just evaluate 2 additional settings.*\n",
    "\n",
    "*For the embedding of the node features, instead of the linear transformation of the node states as suggested in the tutorial, try to add one fully connected layer with ReLU activation and one additional fully connected layer.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288b94f",
   "metadata": {},
   "source": [
    "#### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d32526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5783b",
   "metadata": {},
   "source": [
    "#### Data Aggregation from Multiple Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36208d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset\"\n",
    "\n",
    "scene_ids = sorted(set(f.split(\".\")[0] for f in os.listdir(DATASET_PATH)))\n",
    "\n",
    "all_nodes = []\n",
    "all_edges = []\n",
    "offset = 0\n",
    "\n",
    "for scene_id in scene_ids:\n",
    "    nodes_path = os.path.join(DATASET_PATH, f\"{scene_id}.nodes\")\n",
    "    edges_path = os.path.join(DATASET_PATH, f\"{scene_id}.edges\")\n",
    "\n",
    "    if not os.path.exists(nodes_path) or not os.path.exists(edges_path):\n",
    "        continue\n",
    "\n",
    "    nodes_df = pd.read_csv(\n",
    "        nodes_path, header=None,\n",
    "        names=[\"node_id\", \"current_x\", \"current_y\", \"prev_x\", \"prev_y\", \"future_x\", \"future_y\"],\n",
    "        na_values=[\"_\", \"NA\", \"NaN\", \"nan\"]\n",
    "    )\n",
    "\n",
    "    nodes_df = nodes_df.dropna(subset=[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"])\n",
    "    nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]] = nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].astype(float)\n",
    "\n",
    "    local_to_global = {nid: i + offset for i, nid in enumerate(nodes_df[\"node_id\"])}\n",
    "    nodes_df[\"global_id\"] = nodes_df[\"node_id\"].map(local_to_global)\n",
    "\n",
    "    edges_df = pd.read_csv(edges_path, header=None, names=[\"target\", \"source\"])\n",
    "    edges_df[\"source\"] = edges_df[\"source\"].map(local_to_global)\n",
    "    edges_df[\"target\"] = edges_df[\"target\"].map(local_to_global)\n",
    "    \n",
    "    reversed_edges = edges_df.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "    full_edges = pd.concat([edges_df, reversed_edges])\n",
    "\n",
    "    all_nodes.append(nodes_df)\n",
    "    all_edges.append(full_edges)\n",
    "\n",
    "    offset += len(nodes_df)\n",
    "    \n",
    "nodes_all = pd.concat(all_nodes).sort_values(\"global_id\")\n",
    "edges_all = pd.concat(all_edges).dropna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f27c30",
   "metadata": {},
   "source": [
    "#### Tensor Creation for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = tf.convert_to_tensor(\n",
    "    nodes_all[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "labels = tf.convert_to_tensor(\n",
    "    nodes_all[[\"future_x\", \"future_y\"]].fillna(0.0).to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "mask = tf.convert_to_tensor(\n",
    "    ~nodes_all[[\"future_x\", \"future_y\"]].isna().any(axis=1),\n",
    "    dtype=tf.bool\n",
    ")\n",
    "\n",
    "edges = tf.convert_to_tensor(\n",
    "    edges_all[[\"target\", \"source\"]].to_numpy(),\n",
    "    dtype=tf.int64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cd81e",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "random_indices = np.random.permutation(len(nodes_all))\n",
    "split_idx = int(split_ratio * len(random_indices))\n",
    "train_indices = random_indices[: split_idx]\n",
    "test_indices = random_indices[split_idx :]\n",
    "\n",
    "train_node_features = node_features.numpy()[train_indices]\n",
    "test_node_features = node_features.numpy()[test_indices]\n",
    "\n",
    "train_labels = labels.numpy()[train_indices]\n",
    "test_labels = labels.numpy()[test_indices]\n",
    "\n",
    "train_mask = mask.numpy()[train_indices]\n",
    "test_mask = mask.numpy()[test_indices]\n",
    "\n",
    "train_node_features = tf.convert_to_tensor(train_node_features, dtype=tf.float32)\n",
    "test_node_features = tf.convert_to_tensor(test_node_features, dtype=tf.float32)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "\n",
    "train_mask = tf.convert_to_tensor(train_mask, dtype=tf.bool)\n",
    "test_mask = tf.convert_to_tensor(test_mask, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e505f",
   "metadata": {},
   "source": [
    "*Display*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363560cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training features:\", train_node_features.shape)\n",
    "print(\"Shape of training labels:\", train_labels.shape)\n",
    "print(\"Shape of training masks:\", train_mask.shape)\n",
    "\n",
    "print(\"Shape of test features:\", test_node_features.shape)\n",
    "print(\"Shape of test labels:\", test_labels.shape)\n",
    "print(\"Shape of test masks:\", test_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7d423",
   "metadata": {},
   "source": [
    "#### Graph Attention Layer Definition (GAT Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        h = tf.matmul(node_states, self.kernel)\n",
    "\n",
    "        edge_states = tf.gather(h, edges)\n",
    "        edge_states = tf.reshape(\n",
    "            edge_states, (tf.shape(edges)[0], 2 * self.units)\n",
    "        )\n",
    "        scores = tf.nn.leaky_relu(\n",
    "            tf.matmul(edge_states, self.kernel_attention)\n",
    "        )\n",
    "        scores = tf.squeeze(scores, -1)\n",
    "        \n",
    "        scores_exp = tf.exp(tf.clip_by_value(scores, -2, 2))\n",
    "        denom = tf.math.unsorted_segment_sum(\n",
    "            data=scores_exp,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        denom_per_edge = tf.gather(denom, edges[:, 0])\n",
    "        alpha = scores_exp / (denom_per_edge + tf.keras.backend.epsilon())\n",
    "        \n",
    "        neigh = tf.gather(h, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=neigh * alpha[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f0afd",
   "metadata": {},
   "source": [
    "#### Define Multi-Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_heads = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        head_outputs = [head([node_states, edges]) for head in self.attention_heads]\n",
    "        if self.merge_type == \"concat\":\n",
    "            h = tf.concat(head_outputs, axis=-1)\n",
    "        else:\n",
    "            h = tf.reduce_mean(tf.stack(head_outputs, axis=-1), axis=-1)\n",
    "        return tf.nn.relu(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d40777",
   "metadata": {},
   "source": [
    "#### Define Graph Attention Network (GAT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320666fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features,\n",
    "        edges,\n",
    "        hidden_units=32,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        output_dim=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "\n",
    "        self.preprocess = keras.Sequential([\n",
    "            layers.Dense(hidden_units * num_heads, activation=\"relu\", name=\"embedding_fc1\"),\n",
    "            layers.Dense(hidden_units * num_heads, name=\"embedding_fc2\"),\n",
    "        ], name=\"node_embedding\")\n",
    "\n",
    "        self.gat_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads, merge_type=\"concat\")\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, edges = inputs\n",
    "\n",
    "        x = self.preprocess(x)\n",
    "\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat([x, edges]) + x\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, y_true = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self([self.node_features, self.edges])\n",
    "            y_pred_batch = tf.gather(y_pred, indices)\n",
    "            loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, y_true = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        y_pred_batch = tf.gather(y_pred, indices)\n",
    "        loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        return tf.gather(y_pred, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e8053",
   "metadata": {},
   "source": [
    "#### Define Training Configuration and Hyperparameter Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 32\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 30\n",
    "\n",
    "HEAD_OPTIONS = [2, 4, 8]\n",
    "\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\",\n",
    "    min_delta=1e-4,\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60dba3",
   "metadata": {},
   "source": [
    "#### Train and Compare GAT Models with Varying Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}\n",
    "mae_results = {}\n",
    "predictions = {}\n",
    "\n",
    "for num_heads in HEAD_OPTIONS:\n",
    "    print(f\"\\n--- Training model with {num_heads} attention heads ---\")\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    gat_model = GraphAttentionNetwork(\n",
    "        node_features=node_features,\n",
    "        edges=edges,\n",
    "        hidden_units=HIDDEN_UNITS,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        output_dim=OUTPUT_DIM\n",
    "    )\n",
    "\n",
    "    gat_model.compile(\n",
    "        loss=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        metrics=[mae_metric]\n",
    "    )\n",
    "\n",
    "    history = gat_model.fit(\n",
    "        x=train_indices,\n",
    "        y=train_labels,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    loss, mae = gat_model.evaluate(\n",
    "        x=test_indices,\n",
    "        y=test_labels,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    histories[num_heads] = history.history\n",
    "    mae_results[num_heads] = mae\n",
    "    predictions[num_heads] = gat_model.predict(x=test_indices)\n",
    "\n",
    "test_preds = gat_model.predict(x=test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf67f06",
   "metadata": {},
   "source": [
    "#### Validation MAE Curves â€” Attention Head Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_heads in HEAD_OPTIONS:\n",
    "    val_mae = histories[num_heads]['val_mae']\n",
    "    plt.plot(val_mae, label=f\"{num_heads} heads\")\n",
    "plt.title(\"Validation MAE per number of attention heads\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9e9ca",
   "metadata": {},
   "source": [
    "#### Test MAE Comparison by Attention Head Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0341126",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.bar(\n",
    "    [str(h) + \" heads\" for h in mae_results.keys()],\n",
    "    [mae_results[h] for h in mae_results],\n",
    "    color=\"skyblue\"\n",
    ")\n",
    "plt.title(\"Test MAE per model\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd39df",
   "metadata": {},
   "source": [
    "*Display*:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb83b3",
   "metadata": {},
   "source": [
    "#### Prediction Samples from Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a916fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_heads = min(mae_results, key=mae_results.get)\n",
    "best_preds = predictions[best_heads]\n",
    "\n",
    "print(f\"\\nBest model: {best_heads} attention heads (Test MAE = {mae_results[best_heads]:.4f})\")\n",
    "\n",
    "errors = []\n",
    "\n",
    "for i, (pred, true) in enumerate(zip(best_preds, test_labels)):\n",
    "    true_norm = tf.norm(true).numpy()\n",
    "    l2_error = tf.norm(pred - true).numpy()\n",
    "    errors.append({\n",
    "        \"index\": i,\n",
    "        \"prediction\": pred,\n",
    "        \"ground_truth\": true,\n",
    "        \"l2_error\": l2_error\n",
    "    })\n",
    "\n",
    "errors_sorted = sorted(errors, key=lambda x: x[\"l2_error\"], reverse=False)\n",
    "\n",
    "num_examples = 5\n",
    "for i, item in enumerate(errors_sorted[:num_examples]):\n",
    "    pred = item[\"prediction\"]\n",
    "    true = item[\"ground_truth\"]\n",
    "    l2_error = item[\"l2_error\"]\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"\\tPrediction   = ({pred[0]:.2f}, {pred[1]:.2f})\")\n",
    "    print(f\"\\tGround Truth = ({true[0]:.2f}, {true[1]:.2f})\")\n",
    "    print(f\"\\tL2 Error     = {l2_error:.2f} units\")\n",
    "    print(\"---\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
