{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9905936d",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## Task 3:\n",
    "Replace the learned attention mechanism with an attention mechanism based on the Cosine similarity between node vectors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288b94f",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d32526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5783b",
   "metadata": {},
   "source": [
    "#### Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36208d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing .nodes and .edges files\n",
    "DATASET_PATH = \"dataset\"\n",
    "\n",
    "# Retrieve all scene identifiers (without extension)\n",
    "scene_ids = sorted(set(f.split(\".\")[0] for f in os.listdir(DATASET_PATH)))\n",
    "\n",
    "# Lists to hold all nodes and edges from all scenes\n",
    "all_nodes = []\n",
    "all_edges = []\n",
    "offset = 0 # Global offset to ensure unique node IDs across scenes\n",
    "\n",
    "for scene_id in scene_ids:\n",
    "    nodes_path = os.path.join(DATASET_PATH, f\"{scene_id}.nodes\")\n",
    "    edges_path = os.path.join(DATASET_PATH, f\"{scene_id}.edges\")\n",
    "    \n",
    "    if not os.path.exists(nodes_path) or not os.path.exists(edges_path):\n",
    "        continue\n",
    "    \n",
    "    # Load node data\n",
    "    nodes_df = pd.read_csv(\n",
    "        nodes_path, header=None,\n",
    "        names=[\"node_id\", \"current_x\", \"current_y\", \"prev_x\", \"prev_y\", \"future_x\", \"future_y\"],\n",
    "        na_values=[\"_\", \"NA\", \"NaN\", \"nan\"]\n",
    "    )\n",
    "    \n",
    "    # Drop rows with missing essential position data\n",
    "    nodes_df = nodes_df.dropna(subset=[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"])\n",
    "    nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]] = nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].astype(float)\n",
    "    \n",
    "    # Map local node IDs to global ones\n",
    "    local_to_global = {nid: i + offset for i, nid in enumerate(nodes_df[\"node_id\"])}\n",
    "    nodes_df[\"global_id\"] = nodes_df[\"node_id\"].map(local_to_global)\n",
    "    \n",
    "    # Load edge data and map local IDs to global IDs\n",
    "    edges_df = pd.read_csv(edges_path, header=None, names=[\"target\", \"source\"])\n",
    "    edges_df[\"source\"] = edges_df[\"source\"].map(local_to_global)\n",
    "    edges_df[\"target\"] = edges_df[\"target\"].map(local_to_global)\n",
    "    \n",
    "    # Add reverse edges to make the graph undirected\n",
    "    reversed_edges = edges_df.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "    full_edges = pd.concat([edges_df, reversed_edges])\n",
    "    \n",
    "    # Accumulate nodes and edges\n",
    "    all_nodes.append(nodes_df)\n",
    "    all_edges.append(full_edges)\n",
    "\n",
    "    offset += len(nodes_df)\n",
    "    \n",
    "# Merge all nodes and edges into global structures\n",
    "nodes_all = pd.concat(all_nodes).sort_values(\"global_id\")\n",
    "edges_all = pd.concat(all_edges).dropna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f27c30",
   "metadata": {},
   "source": [
    "#### Create TensorFlow Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow tensors\n",
    "node_features = tf.convert_to_tensor(\n",
    "    nodes_all[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "labels = tf.convert_to_tensor(\n",
    "    nodes_all[[\"future_x\", \"future_y\"]].fillna(0.0).to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "mask = tf.convert_to_tensor(\n",
    "    ~nodes_all[[\"future_x\", \"future_y\"]].isna().any(axis=1),\n",
    "    dtype=tf.bool\n",
    ")\n",
    "\n",
    "edges = tf.convert_to_tensor(\n",
    "    edges_all[[\"target\", \"source\"]].to_numpy(),\n",
    "    dtype=tf.int64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cd81e",
   "metadata": {},
   "source": [
    "#### Split Dataset into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets (80/20 split)\n",
    "split_ratio = 0.5\n",
    "random_indices = np.random.permutation(len(nodes_all))\n",
    "split_idx = int(split_ratio * len(random_indices))\n",
    "train_indices = random_indices[: split_idx]\n",
    "test_indices = random_indices[split_idx :]\n",
    "\n",
    "# Subset features and labels for training and testing\n",
    "train_node_features = node_features.numpy()[train_indices]\n",
    "test_node_features = node_features.numpy()[test_indices]\n",
    "\n",
    "train_labels = labels.numpy()[train_indices]\n",
    "test_labels = labels.numpy()[test_indices]\n",
    "\n",
    "train_mask = mask.numpy()[train_indices]\n",
    "test_mask = mask.numpy()[test_indices]\n",
    "\n",
    "# Convert subsets back to TensorFlow tensors\n",
    "train_node_features = tf.convert_to_tensor(train_node_features, dtype=tf.float32)\n",
    "test_node_features = tf.convert_to_tensor(test_node_features, dtype=tf.float32)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "\n",
    "train_mask = tf.convert_to_tensor(train_mask, dtype=tf.bool)\n",
    "test_mask = tf.convert_to_tensor(test_mask, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e505f",
   "metadata": {},
   "source": [
    "Print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363560cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training features:\", train_node_features.shape)\n",
    "print(\"Shape of training labels:\", train_labels.shape)\n",
    "print(\"Shape of training masks:\", train_mask.shape)\n",
    "\n",
    "print(\"Shape of test features:\", test_node_features.shape)\n",
    "print(\"Shape of test labels:\", test_labels.shape)\n",
    "print(\"Shape of test masks:\", test_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7d423",
   "metadata": {},
   "source": [
    "#### Define GAT Layer: GraphAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # input_shape[0] is (num_nodes, feature_dim)\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        # 1) Linear transform\n",
    "        h = tf.matmul(node_states, self.kernel)  # (N, units)\n",
    "        \n",
    "        # 2) Gather h_target and h_source\n",
    "        h_target = tf.gather(h, edges[:, 0])  # (E, units)\n",
    "        h_source = tf.gather(h, edges[:, 1])  # (E, units)\n",
    "        \n",
    "        # 3) Cosine similarity\n",
    "        dot_prod = tf.reduce_sum(h_target * h_source, axis=1)  # (E,)\n",
    "        norm_target = tf.norm(h_target, axis=1)\n",
    "        norm_source = tf.norm(h_source, axis=1)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        cosine_sim = dot_prod / (norm_target * norm_source + epsilon)  # (E,)\n",
    "        \n",
    "        # 4) Normalize the attention scores per target node\n",
    "        scores_exp = tf.exp(cosine_sim)\n",
    "        denom = tf.math.unsorted_segment_sum(\n",
    "            data=scores_exp,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        # broadcast denom back to each edge\n",
    "        denom_per_edge = tf.gather(denom, edges[:, 0])\n",
    "        alpha = scores_exp / (denom_per_edge + epsilon)\n",
    "        \n",
    "        # 5) Weighted aggregation of neighbor states\n",
    "        neigh = tf.gather(h, edges[:, 1])             # (E, units)\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=neigh * alpha[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f0afd",
   "metadata": {},
   "source": [
    "#### Define Multi-Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        # create one GraphAttention per head\n",
    "        self.attention_heads = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        head_outputs = [head([node_states, edges]) for head in self.attention_heads]\n",
    "        if self.merge_type == \"concat\":\n",
    "            h = tf.concat(head_outputs, axis=-1)  # (N, units * num_heads)\n",
    "        else:\n",
    "            h = tf.reduce_mean(tf.stack(head_outputs, axis=-1), axis=-1)  # (N, units)\n",
    "        return tf.nn.relu(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d40777",
   "metadata": {},
   "source": [
    "#### Define Graph Attention Network (GAT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320666fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features,\n",
    "        edges,\n",
    "        hidden_units=32,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        output_dim=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        # fixed graph inputs\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        # initial linear projection\n",
    "        self.preprocess = keras.Sequential([\n",
    "            layers.Dense(hidden_units * num_heads, activation=\"relu\"),\n",
    "            layers.Dense(hidden_units * num_heads, activation=\"relu\"),\n",
    "        ])\n",
    "        # stack of multi‑head GAT layers\n",
    "        self.gat_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads, merge_type=\"concat\")\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        # final regression head\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, edges = inputs\n",
    "        x = self.preprocess(x)\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat([x, edges]) + x   # residual connection\n",
    "        return self.output_layer(x)   # (N, 2)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, y_true = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self([self.node_features, self.edges])\n",
    "            # gather only the nodes in this batch\n",
    "            y_pred_batch = tf.gather(y_pred, indices)\n",
    "            loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, y_true = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        y_pred_batch = tf.gather(y_pred, indices)\n",
    "        loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        return tf.gather(y_pred, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e8053",
   "metadata": {},
   "source": [
    "#### Set Training Parameters and Compile Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HIDDEN_UNITS = 32         # GAT head size\n",
    "NUM_LAYERS = 2            # number of GAT layers\n",
    "OUTPUT_DIM = 2            # (future_x, future_y)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 1e-4      # more stable for regression\n",
    "PATIENCE = 30\n",
    "\n",
    "HEAD_OPTIONS = [2, 4, 8]\n",
    "\n",
    "# Loss and metric definitions\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\",\n",
    "    min_delta=1e-4,\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60dba3",
   "metadata": {},
   "source": [
    "#### Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}\n",
    "mae_results = {}\n",
    "predictions = {}\n",
    "\n",
    "for num_heads in HEAD_OPTIONS:\n",
    "    print(f\"\\n--- Training model with {num_heads} attention heads ---\")\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    # Create GAT model\n",
    "    gat_model = GraphAttentionNetwork(\n",
    "        node_features=node_features,\n",
    "        edges=edges,\n",
    "        hidden_units=HIDDEN_UNITS,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        output_dim=OUTPUT_DIM\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    gat_model.compile(\n",
    "        loss=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        metrics=[mae_metric]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = gat_model.fit(\n",
    "        x=train_indices,\n",
    "        y=train_labels,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Evaluate on test data\n",
    "    loss, mae = gat_model.evaluate(\n",
    "        x=test_indices,\n",
    "        y=test_labels,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Save the results\n",
    "    histories[num_heads] = history.history\n",
    "    mae_results[num_heads] = mae\n",
    "    predictions[num_heads] = gat_model.predict(x=test_indices)\n",
    "\n",
    "# Get predicted future positions\n",
    "test_preds = gat_model.predict(x=test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf67f06",
   "metadata": {},
   "source": [
    "#### Visualize Validation MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MAE curves on the validation set\n",
    "plt.figure(figsize=(10, 6))\n",
    "for num_heads in HEAD_OPTIONS:\n",
    "    val_mae = histories[num_heads]['val_mae']\n",
    "    plt.plot(val_mae, label=f\"{num_heads} heads\")\n",
    "plt.title(\"Validation MAE per number of attention heads\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9e9ca",
   "metadata": {},
   "source": [
    "#### Bar Chart of Test MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0341126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Bar chart of test MAE\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.bar(\n",
    "    [str(h) + \" heads\" for h in mae_results.keys()],\n",
    "    [mae_results[h] for h in mae_results],\n",
    "    color=\"skyblue\"\n",
    ")\n",
    "plt.title(\"Test MAE per model\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb83b3",
   "metadata": {},
   "source": [
    "#### Prediction Samples from Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a916fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualization of a few predictions vs ground truth for the best model\n",
    "best_heads = min(mae_results, key=mae_results.get)\n",
    "best_preds = predictions[best_heads]\n",
    "\n",
    "print(f\"\\nBest model: {best_heads} attention heads (Test MAE = {mae_results[best_heads]:.4f})\")\n",
    "\n",
    "# Display a few predictions\n",
    "num_examples = 5\n",
    "for i in range(num_examples):\n",
    "    pred = best_preds[i]\n",
    "    true = test_labels[i]\n",
    "    l2_error = tf.norm(pred - true).numpy()\n",
    "    true_norm = tf.norm(true).numpy()\n",
    "    percent_error = (l2_error / (true_norm + 1e-8)) * 100\n",
    "\n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(f\"  Prediction   : ({pred[0]:.2f}, {pred[1]:.2f})\")\n",
    "    print(f\"  Ground Truth : ({true[0]:.2f}, {true[1]:.2f})\")\n",
    "    print(f\"  L2 Error     : {l2_error:.2f}\")\n",
    "    print(f\"  % Error      : {percent_error:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
