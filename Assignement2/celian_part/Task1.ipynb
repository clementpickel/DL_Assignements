{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb9030a",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## Task 1:\n",
    "Adjust the tutorial implementation to perform the given prediction task and perform a suitable evaluation on a dedicated test set.\n",
    "\n",
    "For example, you can compute the Euclidean distance between the target point and the predicted point.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f7fdc",
   "metadata": {},
   "source": [
    "#### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c011f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e095e",
   "metadata": {},
   "source": [
    "#### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing .nodes and .edges files\n",
    "DATASET_PATH = \"dataset\"\n",
    "\n",
    "# Retrieve all scene identifiers (without extension)\n",
    "scene_ids = sorted(set(f.split(\".\")[0] for f in os.listdir(DATASET_PATH)))\n",
    "\n",
    "# Lists to hold all nodes and edges from all scenes\n",
    "all_nodes = []\n",
    "all_edges = []\n",
    "offset = 0 # Global offset to ensure unique node IDs across scenes\n",
    "\n",
    "for scene_id in scene_ids:\n",
    "    nodes_path = os.path.join(DATASET_PATH, f\"{scene_id}.nodes\")\n",
    "    edges_path = os.path.join(DATASET_PATH, f\"{scene_id}.edges\")\n",
    "    \n",
    "    if not os.path.exists(nodes_path) or not os.path.exists(edges_path):\n",
    "        continue\n",
    "    \n",
    "    # Load node data\n",
    "    nodes_df = pd.read_csv(\n",
    "        nodes_path, header=None,\n",
    "        names=[\"node_id\", \"current_x\", \"current_y\", \"prev_x\", \"prev_y\", \"future_x\", \"future_y\"],\n",
    "        na_values=[\"_\", \"NA\", \"NaN\", \"nan\"]\n",
    "    )\n",
    "    \n",
    "    # Drop rows with missing essential position data\n",
    "    nodes_df = nodes_df.dropna(subset=[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"])\n",
    "    nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]] = nodes_df[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].astype(float)\n",
    "    \n",
    "    # Map local node IDs to global ones\n",
    "    local_to_global = {nid: i + offset for i, nid in enumerate(nodes_df[\"node_id\"])}\n",
    "    nodes_df[\"global_id\"] = nodes_df[\"node_id\"].map(local_to_global)\n",
    "    \n",
    "    # Load edge data and map local IDs to global IDs\n",
    "    edges_df = pd.read_csv(edges_path, header=None, names=[\"target\", \"source\"])\n",
    "    edges_df[\"source\"] = edges_df[\"source\"].map(local_to_global)\n",
    "    edges_df[\"target\"] = edges_df[\"target\"].map(local_to_global)\n",
    "    \n",
    "    # Add reverse edges to make the graph undirected\n",
    "    reversed_edges = edges_df.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "    full_edges = pd.concat([edges_df, reversed_edges])\n",
    "    \n",
    "    # Accumulate nodes and edges\n",
    "    all_nodes.append(nodes_df)\n",
    "    all_edges.append(full_edges)\n",
    "\n",
    "    offset += len(nodes_df)\n",
    "    \n",
    "# Merge all nodes and edges into global structures\n",
    "nodes_all = pd.concat(all_nodes).sort_values(\"global_id\")\n",
    "edges_all = pd.concat(all_edges).dropna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fcf35a",
   "metadata": {},
   "source": [
    "#### TensorFlow Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423faee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow tensors\n",
    "node_features = tf.convert_to_tensor(\n",
    "    nodes_all[[\"current_x\", \"current_y\", \"prev_x\", \"prev_y\"]].to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "labels = tf.convert_to_tensor(\n",
    "    nodes_all[[\"future_x\", \"future_y\"]].fillna(0.0).to_numpy(),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "mask = tf.convert_to_tensor(\n",
    "    ~nodes_all[[\"future_x\", \"future_y\"]].isna().any(axis=1),\n",
    "    dtype=tf.bool\n",
    ")\n",
    "\n",
    "edges = tf.convert_to_tensor(\n",
    "    edges_all[[\"target\", \"source\"]].to_numpy(),\n",
    "    dtype=tf.int64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dae595",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets (50/50 split)\n",
    "split_ratio = 0.5\n",
    "random_indices = np.random.permutation(len(nodes_all))\n",
    "split_idx = int(split_ratio * len(random_indices))\n",
    "train_indices = random_indices[: split_idx]\n",
    "test_indices = random_indices[split_idx :]\n",
    "\n",
    "# Subset features and labels for training and testing\n",
    "train_node_features = node_features.numpy()[train_indices]\n",
    "test_node_features = node_features.numpy()[test_indices]\n",
    "\n",
    "train_labels = labels.numpy()[train_indices]\n",
    "test_labels = labels.numpy()[test_indices]\n",
    "\n",
    "train_mask = mask.numpy()[train_indices]\n",
    "test_mask = mask.numpy()[test_indices]\n",
    "\n",
    "# Convert subsets back to TensorFlow tensors\n",
    "train_node_features = tf.convert_to_tensor(train_node_features, dtype=tf.float32)\n",
    "test_node_features = tf.convert_to_tensor(test_node_features, dtype=tf.float32)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "\n",
    "train_mask = tf.convert_to_tensor(train_mask, dtype=tf.bool)\n",
    "test_mask = tf.convert_to_tensor(test_mask, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5baff1",
   "metadata": {},
   "source": [
    "Print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training features:\", train_node_features.shape)\n",
    "print(\"Shape of training labels:\", train_labels.shape)\n",
    "print(\"Shape of training masks:\", train_mask.shape)\n",
    "\n",
    "print(\"Shape of test features:\", test_node_features.shape)\n",
    "print(\"Shape of test labels:\", test_labels.shape)\n",
    "print(\"Shape of test masks:\", test_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36c68e",
   "metadata": {},
   "source": [
    "#### Graph Attention Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # input_shape[0] is (num_nodes, feature_dim)\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        # attention kernel takes concatenated pair [h_i || h_j]\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        # 1) Linear transform\n",
    "        h = tf.matmul(node_states, self.kernel)  # (N, units)\n",
    "\n",
    "        # 2) Compute attention scores for each edge\n",
    "        #    gather [h_target, h_source]\n",
    "        edge_states = tf.gather(h, edges)            # (E, 2, units)\n",
    "        edge_states = tf.reshape(\n",
    "            edge_states, (tf.shape(edges)[0], 2 * self.units)\n",
    "        )\n",
    "        scores = tf.nn.leaky_relu(\n",
    "            tf.matmul(edge_states, self.kernel_attention)\n",
    "        )  # (E,1)\n",
    "        scores = tf.squeeze(scores, -1)               # (E,)\n",
    "        \n",
    "        # 3) Normalize per target node\n",
    "        scores_exp = tf.exp(tf.clip_by_value(scores, -2, 2))\n",
    "        denom = tf.math.unsorted_segment_sum(\n",
    "            data=scores_exp,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        # broadcast denom back to each edge\n",
    "        denom_per_edge = tf.gather(denom, edges[:, 0])\n",
    "        alpha = scores_exp / (denom_per_edge + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # 4) Weighted aggregation of neighbor states\n",
    "        neigh = tf.gather(h, edges[:, 1])             # (E, units)\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=neigh * alpha[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0]\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10357b9",
   "metadata": {},
   "source": [
    "#### Multi-Head Graph Attention Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27518948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        # create one GraphAttention per head\n",
    "        self.attention_heads = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        head_outputs = [head([node_states, edges]) for head in self.attention_heads]\n",
    "        if self.merge_type == \"concat\":\n",
    "            h = tf.concat(head_outputs, axis=-1)  # (N, units * num_heads)\n",
    "        else:\n",
    "            h = tf.reduce_mean(tf.stack(head_outputs, axis=-1), axis=-1)  # (N, units)\n",
    "        return tf.nn.relu(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14a168",
   "metadata": {},
   "source": [
    "#### Graph Attention Network Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features,\n",
    "        edges,\n",
    "        hidden_units=32,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        output_dim=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        # fixed graph inputs\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        # initial linear projection\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        # stack of multi‑head GAT layers\n",
    "        self.gat_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads, merge_type=\"concat\")\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        # final regression head\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, edges = inputs\n",
    "        x = self.preprocess(x)\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat([x, edges]) + x   # residual connection\n",
    "        return self.output_layer(x)   # (N, 2)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, y_true = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self([self.node_features, self.edges])\n",
    "            # gather only the nodes in this batch\n",
    "            y_pred_batch = tf.gather(y_pred, indices)\n",
    "            loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, y_true = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        y_pred_batch = tf.gather(y_pred, indices)\n",
    "        loss = self.compiled_loss(y_true, y_pred_batch)\n",
    "        self.compiled_metrics.update_state(y_true, y_pred_batch)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        y_pred = self([self.node_features, self.edges])\n",
    "        return tf.gather(y_pred, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e925f0b",
   "metadata": {},
   "source": [
    "#### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HIDDEN_UNITS = 32         # GAT head size\n",
    "NUM_HEADS = 4             # number of attention heads\n",
    "NUM_LAYERS = 2            # number of GAT layers\n",
    "OUTPUT_DIM = 2            # (future_x, future_y)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 1e-3      # more stable for regression\n",
    "PATIENCE = 10\n",
    "\n",
    "# Loss and metric definitions\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\",\n",
    "    min_delta=1e-4,\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Create GAT model\n",
    "gat_model = GraphAttentionNetwork(\n",
    "    node_features=node_features,\n",
    "    edges=edges,\n",
    "    hidden_units=HIDDEN_UNITS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    output_dim=OUTPUT_DIM\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "gat_model.compile(\n",
    "    loss=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[mae_metric]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gat_model.fit(\n",
    "    x=train_indices,\n",
    "    y=train_labels,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "loss, mae = gat_model.evaluate(\n",
    "    x=test_indices,\n",
    "    y=test_labels,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98105197",
   "metadata": {},
   "source": [
    "Display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41fd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTest MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9bfb1",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e82085",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = gat_model.predict(x=test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993c2a6",
   "metadata": {},
   "source": [
    "Display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 10 predictions with ground truth\n",
    "for i, (pred, true) in enumerate(zip(test_preds[:10], test_labels[:10])):\n",
    "    true_norm = tf.norm(true).numpy()\n",
    "    l2_error = tf.norm(pred - true).numpy()\n",
    "    percent_error = (l2_error / true_norm) * 100\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"\\tPrediction   = ({pred[0]:.2f}, {pred[1]:.2f})\")\n",
    "    print(f\"\\tGround Truth = ({true[0]:.2f}, {true[1]:.2f})\")\n",
    "    print(f\"\\tL2 Error     = {l2_error:.2f} units\")\n",
    "    print(f\"\\tError %%      = {percent_error:.2f}%\")\n",
    "    print(\"---\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
