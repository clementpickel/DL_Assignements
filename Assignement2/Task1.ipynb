{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd2b827",
   "metadata": {},
   "source": [
    "**Task 1** of your assignment is asking to **modify the Graph Attention Network (GAT) tutorial** so that it performs a **trajectory prediction task**, specifically:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ **What You Need to Do in Task 1:**\n",
    "\n",
    "#### 1. **Adapt the GAT Tutorial to Predict Future Positions**\n",
    "Instead of classifying node labels (like in the original tutorial), you now need to **predict the future (x, y) position** of a pedestrian, given their current and previous locations, and their connections to other pedestrians (edges).\n",
    "\n",
    "- Each **node** in the graph is a pedestrian.\n",
    "- Each **edge** represents a relationship (like proximity or visibility) between pedestrians.\n",
    "- Each **node has features** like:\n",
    "  - `current x`, `current y`\n",
    "  - `previous x`, `previous y`\n",
    "- Your **goal** is to predict:\n",
    "  - `future x`, `future y`\n",
    "\n",
    "> This turns the problem from **classification** into a **regression** task (predicting continuous values).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Use Graph Structure**\n",
    "Use the `.edges` files to build the graph â€” optionally making it **bidirectional** or **visibility-based** as explained in the instructions.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Train Your Model and Evaluate**\n",
    "Train your GAT model on a training set and evaluate it on a **separate test set** by:\n",
    "\n",
    "- Predicting future positions for each pedestrian in the test set.\n",
    "- Comparing predicted vs actual using **Euclidean distance**:\n",
    "\n",
    "```python\n",
    "euclidean_distance = np.sqrt((pred_x - true_x)**2 + (pred_y - true_y)**2)\n",
    "```\n",
    "\n",
    "Youâ€™ll probably average that over all test nodes to report a single number like:\n",
    "\n",
    "```python\n",
    "mean_distance_error = np.mean(euclidean_distances)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Task 1\n",
    "\n",
    "| Aspect | What to do |\n",
    "|--------|------------|\n",
    "| **Data** | Load `.nodes` and `.edges` files for scenes |\n",
    "| **Model** | Modify GAT to output 2 values per node: `future x`, `future y` |\n",
    "| **Loss** | Use **Mean Squared Error (MSE)** or **Euclidean distance loss** |\n",
    "| **Eval** | Measure average Euclidean distance between prediction and ground truth on test set |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc40c85",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a03a7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 6)\n",
    "pd.set_option(\"display.max_rows\", 6)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d47af",
   "metadata": {},
   "source": [
    "## Get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4bb4c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_files = glob.glob(os.path.join(\"dataset\", \"*.edges\"))\n",
    "node_files = glob.glob(os.path.join(\"dataset\", \"*.nodes\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea80240",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "75d85a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dfs = []\n",
    "for file_path in edge_files:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep=\", \",\n",
    "        header=None,\n",
    "        names=[\"target\", \"source\"],\n",
    "        na_values=\"-1\"\n",
    "    )\n",
    "    edge_dfs.append(df)\n",
    "\n",
    "node_dfs = []\n",
    "for file_path in node_files:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep=\",\",\n",
    "        header=None,\n",
    "        names=[\"node id\", \"current x\", \"current y\", \"previous x\", \"previous y\", \"future x\", \"future y\"],\n",
    "        na_values=\"_\"\n",
    "    )\n",
    "    node_dfs.append(df)\n",
    "\n",
    "edges = pd.concat(edge_dfs, ignore_index=True)\n",
    "nodes = pd.concat(node_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9cda7445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        target      source\n",
      "0     19585800  19590700.0\n",
      "1     19585800  19595200.0\n",
      "2     19590700  19592400.0\n",
      "...        ...         ...\n",
      "3456  19592800         NaN\n",
      "3457  20014600         NaN\n",
      "3458  20015100         NaN\n",
      "\n",
      "[3459 rows x 2 columns]\n",
      "       node id  current x  current y  ...  previous y  future x  future y\n",
      "0     19502500    40972.0   -16957.0  ...    -16957.0   41185.0  -16480.0\n",
      "1     19585800    12688.0    -6816.0  ...     -6816.0   13381.0   -7427.0\n",
      "2     19590400   -16367.0    21644.0  ...     21644.0       NaN       NaN\n",
      "...        ...        ...        ...  ...         ...       ...       ...\n",
      "2306  20015100   -19196.0     3668.0  ...      3041.0  -19196.0    3668.0\n",
      "2307  20015600    17568.0   -13258.0  ...    -14736.0   17568.0  -13258.0\n",
      "2308  20015900    16994.0   -12152.0  ...         NaN   16994.0  -12152.0\n",
      "\n",
      "[2309 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(edges)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501960e",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1941c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prev_x_diff = nodes[\"current x\"] - nodes[\"previous x\"]\n",
    "prev_y_diff = nodes[\"current y\"] - nodes[\"previous y\"]\n",
    "\n",
    "nodes[\"previous vector x\"] = prev_x_diff\n",
    "nodes[\"previous vector y\"] = prev_y_diff\n",
    "\n",
    "fut_x_diff = nodes[\"future x\"] - nodes[\"current x\"]\n",
    "fut_y_diff = nodes[\"future y\"] - nodes[\"current y\"]\n",
    "\n",
    "nodes[\"future vector x\"] = fut_x_diff # it's easier to predict a relative value\n",
    "nodes[\"future vector y\"] = fut_y_diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4956efe",
   "metadata": {},
   "source": [
    "### Remove edge without node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "322f7cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19585800</td>\n",
       "      <td>19590700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19585800</td>\n",
       "      <td>19595200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19590700</td>\n",
       "      <td>19592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>20002900</td>\n",
       "      <td>20004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3453</th>\n",
       "      <td>20013400</td>\n",
       "      <td>20013402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>20015600</td>\n",
       "      <td>20015900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2931 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target    source\n",
       "0     19585800  19590700\n",
       "1     19585800  19595200\n",
       "2     19590700  19592400\n",
       "...        ...       ...\n",
       "3452  20002900  20004700\n",
       "3453  20013400  20013402\n",
       "3454  20015600  20015900\n",
       "\n",
       "[2931 rows x 2 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edges_id = pd.concat([edges['target'], edges[\"source\"]]).dropna().unique()\n",
    "# nodes_filtered = nodes[nodes[\"node id\"].isin(edges_id)]\n",
    "# nodes_filtered\n",
    "\n",
    "nodes_id = nodes[\"node id\"].unique()\n",
    "edges[edges[\"target\"].isin(nodes_id)]\n",
    "edges[edges[\"source\"].isin(nodes_id)]\n",
    "edges = edges.dropna()\n",
    "edges = edges.astype(int)\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fdcaf9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node id</th>\n",
       "      <th>current x</th>\n",
       "      <th>current y</th>\n",
       "      <th>...</th>\n",
       "      <th>future y</th>\n",
       "      <th>future vector x</th>\n",
       "      <th>future vector y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19502500</td>\n",
       "      <td>40972.0</td>\n",
       "      <td>-16957.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-16480.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19585800</td>\n",
       "      <td>12688.0</td>\n",
       "      <td>-6816.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7427.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>-611.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19590400</td>\n",
       "      <td>-16367.0</td>\n",
       "      <td>21644.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>20015100</td>\n",
       "      <td>-19196.0</td>\n",
       "      <td>3668.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3668.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>20015600</td>\n",
       "      <td>17568.0</td>\n",
       "      <td>-13258.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13258.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>20015900</td>\n",
       "      <td>16994.0</td>\n",
       "      <td>-12152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-12152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2309 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       node id  current x  current y  ...  future y  future vector x  \\\n",
       "0     19502500    40972.0   -16957.0  ...  -16480.0            213.0   \n",
       "1     19585800    12688.0    -6816.0  ...   -7427.0            693.0   \n",
       "2     19590400   -16367.0    21644.0  ...       NaN              NaN   \n",
       "...        ...        ...        ...  ...       ...              ...   \n",
       "2306  20015100   -19196.0     3668.0  ...    3668.0              0.0   \n",
       "2307  20015600    17568.0   -13258.0  ...  -13258.0              0.0   \n",
       "2308  20015900    16994.0   -12152.0  ...  -12152.0              0.0   \n",
       "\n",
       "      future vector y  \n",
       "0               477.0  \n",
       "1              -611.0  \n",
       "2                 NaN  \n",
       "...               ...  \n",
       "2306              0.0  \n",
       "2307              0.0  \n",
       "2308              0.0  \n",
       "\n",
       "[2309 rows x 11 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moves future x and future y to last position in column\n",
    "last = [\"future x\", \"future y\", \"future vector x\", \"future vector y\"]\n",
    "cols = [col for col in nodes.columns if col not in last] + last\n",
    "nodes = nodes[cols]\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487439ec",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8beca6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain random indices\n",
    "random_indices = np.random.permutation(range(nodes.shape[0]))\n",
    "\n",
    "# 50/50 split\n",
    "train_data = nodes.iloc[random_indices[: len(random_indices) // 2]]\n",
    "test_data = nodes.iloc[random_indices[len(random_indices) // 2 :]]\n",
    "\n",
    "# Identify test rows with missing future values\n",
    "missing_future_mask = test_data[[\"future x\", \"future y\"]].isna().any(axis=1)\n",
    "test_rows_with_missing = test_data[missing_future_mask]\n",
    "\n",
    "# Remove them from test_data and add to train_data\n",
    "test_data = test_data[~missing_future_mask]\n",
    "train_data = pd.concat([train_data, test_rows_with_missing], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "43a90b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       node id  current x  current y  ...  future y  future vector x  \\\n",
      "0     19591900     9019.0    -4136.0  ...   -4722.0            574.0   \n",
      "1     20000700    40669.0   -18422.0  ...  -18350.0           1145.0   \n",
      "2     19594200     8864.0    -3571.0  ...   -3939.0            276.0   \n",
      "...        ...        ...        ...  ...       ...              ...   \n",
      "1180  19591900    46667.0   -23477.0  ...       NaN              NaN   \n",
      "1181  20000100    22475.0   -13481.0  ...       NaN              NaN   \n",
      "1182  19594200    12262.0    -6186.0  ...       NaN              NaN   \n",
      "\n",
      "      future vector y  \n",
      "0              -586.0  \n",
      "1                72.0  \n",
      "2              -368.0  \n",
      "...               ...  \n",
      "1180              NaN  \n",
      "1181              NaN  \n",
      "1182              NaN  \n",
      "\n",
      "[1183 rows x 11 columns]\n",
      "       node id  current x  current y  ...  future y  future vector x  \\\n",
      "544   19585800    19495.0   -12645.0  ...  -13083.0            929.0   \n",
      "2283  20013400    38201.0   -19607.0  ...  -19822.0            403.0   \n",
      "2182  20002900    34701.0   -21854.0  ...  -21578.0            482.0   \n",
      "...        ...        ...        ...  ...       ...              ...   \n",
      "674   19595200    26107.0   -15488.0  ...  -16132.0           1559.0   \n",
      "1099  19592201    28390.0   -17190.0  ...  -17417.0            902.0   \n",
      "1608  19502500    41211.0   -16663.0  ...  -16643.0             82.0   \n",
      "\n",
      "      future vector y  \n",
      "544            -438.0  \n",
      "2283           -215.0  \n",
      "2182            276.0  \n",
      "...               ...  \n",
      "674            -644.0  \n",
      "1099           -227.0  \n",
      "1608             20.0  \n",
      "\n",
      "[1126 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd223d28",
   "metadata": {},
   "source": [
    "## Prepare the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e3ad5c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges shape:\t\t (2931, 2)\n",
      "Node features shape: (2309, 6)\n",
      "tf.Tensor(\n",
      "[[19585800 19590700]\n",
      " [19585800 19595200]\n",
      " [19590700 19592400]\n",
      " ...\n",
      " [20002900 20004700]\n",
      " [20013400 20013402]\n",
      " [20015600 20015900]], shape=(2931, 2), dtype=int64)\n",
      "       node id  current x  current y  ...  future y  future vector x  \\\n",
      "0     19502500    40972.0   -16957.0  ...  -16480.0            213.0   \n",
      "1     19585800    12688.0    -6816.0  ...   -7427.0            693.0   \n",
      "2     19590400   -16367.0    21644.0  ...       NaN              NaN   \n",
      "...        ...        ...        ...  ...       ...              ...   \n",
      "2306  20015100   -19196.0     3668.0  ...    3668.0              0.0   \n",
      "2307  20015600    17568.0   -13258.0  ...  -13258.0              0.0   \n",
      "2308  20015900    16994.0   -12152.0  ...  -12152.0              0.0   \n",
      "\n",
      "      future vector y  \n",
      "0               477.0  \n",
      "1              -611.0  \n",
      "2                 NaN  \n",
      "...               ...  \n",
      "2306              0.0  \n",
      "2307              0.0  \n",
      "2308              0.0  \n",
      "\n",
      "[2309 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Obtain paper indices which will be used to gather node states\n",
    "# from the graph later on when training the model\n",
    "train_indices = train_data[\"node id\"].to_numpy()\n",
    "test_indices = test_data[\"node id\"].to_numpy()\n",
    "\n",
    "# Obtain ground truth labels corresponding to each paper_id\n",
    "train_labels = train_data[[\"future vector x\", \"future vector y\"]].to_numpy()\n",
    "test_labels = test_data[[\"future vector x\", \"future vector y\"]].to_numpy()\n",
    "\n",
    "# Define graph, namely an edge tensor and a node feature tensor\n",
    "edges = tf.convert_to_tensor(edges[[\"target\", \"source\"]])\n",
    "node_states = tf.convert_to_tensor(nodes.sort_values(\"node id\").iloc[:, 1:-4]) # remove all future values\n",
    "\n",
    "# Print shapes of the graph\n",
    "print(\"Edges shape:\\t\\t\", edges.shape)\n",
    "print(\"Node features shape:\", node_states.shape)\n",
    "print(edges)\n",
    "print(nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7d22b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        # Linearly transform node states\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "\n",
    "        # (1) Compute pair-wise attention scores\n",
    "        node_states_expanded = tf.gather(node_states_transformed, edges)\n",
    "        node_states_expanded = tf.reshape(\n",
    "            node_states_expanded, (tf.shape(edges)[0], -1)\n",
    "        )\n",
    "        attention_scores = tf.nn.leaky_relu(\n",
    "            tf.matmul(node_states_expanded, self.kernel_attention)\n",
    "        )\n",
    "        attention_scores = tf.squeeze(attention_scores, -1)\n",
    "\n",
    "        # (2) Normalize attention scores\n",
    "        attention_scores = tf.math.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
    "        attention_scores_sum = tf.math.unsorted_segment_sum(\n",
    "            data=attention_scores,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.reduce_max(edges[:, 0]) + 1,\n",
    "        )\n",
    "        attention_scores_sum = tf.repeat(\n",
    "            attention_scores_sum, tf.math.bincount(tf.cast(edges[:, 0], \"int32\"))\n",
    "        )\n",
    "        attention_scores_norm = attention_scores / attention_scores_sum\n",
    "\n",
    "        # (3) Gather node states of neighbors, apply attention scores and aggregate\n",
    "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * attention_scores_norm[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0],\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, pair_indices = inputs\n",
    "\n",
    "        # Obtain outputs from each attention head\n",
    "        outputs = [\n",
    "            attention_layer([atom_features, pair_indices])\n",
    "            for attention_layer in self.attention_layers\n",
    "        ]\n",
    "        # Concatenate or average the node states from each head\n",
    "        if self.merge_type == \"concat\":\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            outputs = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        # Activate and return node states\n",
    "        return tf.nn.relu(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5936343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_states,\n",
    "        edges,\n",
    "        hidden_units,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        output_dim,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_states = node_states\n",
    "        self.edges = edges\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        self.attention_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        x = self.preprocess(node_states)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer([x, edges]) + x\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            outputs = self([self.node_states, self.edges])\n",
    "            # Compute loss\n",
    "            loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        # Apply gradients (update weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute probabilities\n",
    "        return tf.nn.softmax(tf.gather(outputs, indices))\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, labels = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute loss\n",
    "        loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec798e",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "99d7d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node graph_attention_network_5_1/multi_head_graph_attention_15_1/graph_attention_122_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3047, in run_cell\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3102, in _run_cell\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3489, in run_ast_nodes\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3549, in run_code\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Temp\\ipykernel_11948\\1091928056.py\", line 29, in <module>\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 371, in fit\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 219, in function\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 113, in one_step_on_data\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Temp\\ipykernel_11948\\3672007774.py\", line 34, in train_step\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 909, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 52, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Temp\\ipykernel_11948\\3672007774.py\", line 25, in call\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 909, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 52, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Temp\\ipykernel_11948\\1482980897.py\", line 82, in call\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 909, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 52, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Temp\\ipykernel_11948\\1482980897.py\", line 39, in call\n\nindices[2852,0] = 20002900 is not in [0, 2309)\n\t [[{{node graph_attention_network_5_1/multi_head_graph_attention_15_1/graph_attention_122_1/GatherV2}}]] [Op:__inference_multi_step_on_iterator_107237]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[189]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m gat_model.compile(loss=loss_fn, optimizer=optimizer, metrics=metrics)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mgat_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shape: (num_nodes, 2)\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVALIDATION_SPLIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     40\u001b[39m loss, mae = gat_model.evaluate(x=test_indices, y=test_labels, verbose=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Graph execution error:\n\nDetected at node graph_attention_network_5_1/multi_head_graph_attention_15_1/graph_attention_122_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3047, in run_cell\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3102, in _run_cell\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3489, in run_ast_nodes\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3549, in run_code\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Temp\\ipykernel_11948\\1091928056.py\", line 29, in <module>\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 371, in fit\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 219, in function\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 113, in one_step_on_data\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Temp\\ipykernel_11948\\3672007774.py\", line 34, in train_step\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 909, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 52, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Temp\\ipykernel_11948\\3672007774.py\", line 25, in call\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 909, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 52, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Temp\\ipykernel_11948\\1482980897.py\", line 82, in call\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 909, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 52, in __call__\n\n  File \"c:\\Users\\cpick\\Documents\\GitHub\\DL_Assignements\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\cpick\\AppData\\Local\\Temp\\ipykernel_11948\\1482980897.py\", line 39, in call\n\nindices[2852,0] = 20002900 is not in [0, 2309)\n\t [[{{node graph_attention_network_5_1/multi_head_graph_attention_15_1/graph_attention_122_1/GatherV2}}]] [Op:__inference_multi_step_on_iterator_107237]"
     ]
    }
   ],
   "source": [
    "# Define hyper-parameters\n",
    "HIDDEN_UNITS = 100\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "OUTPUT_DIM = 2  # Predict future x and y\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 3e-1\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# Loss and metrics for regression\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.SGD(LEARNING_RATE, momentum=MOMENTUM)\n",
    "metrics = [keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=1e-5, patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Build and compile model\n",
    "gat_model = GraphAttentionNetwork(\n",
    "    node_states, edges, HIDDEN_UNITS, NUM_HEADS, NUM_LAYERS, OUTPUT_DIM\n",
    ")\n",
    "gat_model.compile(loss=loss_fn, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Train\n",
    "gat_model.fit(\n",
    "    x=train_indices,\n",
    "    y=train_labels,  # shape: (num_nodes, 2)\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, mae = gat_model.evaluate(x=test_indices, y=test_labels, verbose=0)\n",
    "print(\"--\" * 38 + f\"\\nTest MAE (distance error): {mae:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
