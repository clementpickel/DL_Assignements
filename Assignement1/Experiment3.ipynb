{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3\n",
    "- Load the saved model and replace the output layer of the model, as well as the first two convolutional layers (keep the weights of all other layers).\n",
    "- Train and evaluate the model on the cats and dogs dataset.\n",
    "\n",
    "## Steps\n",
    "Load the saved model from a file.\n",
    "\n",
    "Freeze all layers except the first two convolutional layers and the output layer.\n",
    "\n",
    "Replace the first two convolutional layers and the output layer.\n",
    "\n",
    "Compile the model with a suitable loss function and optimizer.\n",
    "\n",
    "Train and evaluate the model on the Cats vs. Dogs dataset.\n",
    "\n",
    "### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "# Load the pre-trained model\n",
    "saved_model_path = \"models/experiment1_model.keras\"\n",
    "model = keras.models.load_model(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        units = 1\n",
    "    else:\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    # Change output layer for multi-class classification (120 dog breeds)\n",
    "    outputs = layers.Dense(units, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh model with the same architecture\n",
    "fresh_model = make_model(input_shape=(180, 180) + (3,), num_classes=1)\n",
    "base_model = keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "base_model.trainable = False \n",
    "\n",
    "# Reset weights for layers 5 to 22\n",
    "for i in range(5, 22):\n",
    "    base_model.layers[i].set_weights(fresh_model.layers[i].get_weights())\n",
    "    base_model.layers[i].trainable = True  # Make sure the layers are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new output layer (Binary Classification)\n",
    "new_output = layers.Dense(1, activation=\"sigmoid\")(base_model.output)\n",
    "\n",
    "# Create a new model\n",
    "new_model = keras.Model(inputs=base_model.input, outputs=new_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: input_layer_1 — Trainable: False\n",
      "1: rescaling_1 — Trainable: False\n",
      "2: conv2d_4 — Trainable: False\n",
      "3: batch_normalization_8 — Trainable: False\n",
      "4: activation_8 — Trainable: False\n",
      "5: activation_9 — Trainable: True\n",
      "6: separable_conv2d_7 — Trainable: True\n",
      "7: batch_normalization_9 — Trainable: True\n",
      "8: activation_10 — Trainable: True\n",
      "9: separable_conv2d_8 — Trainable: True\n",
      "10: batch_normalization_10 — Trainable: True\n",
      "11: max_pooling2d_3 — Trainable: True\n",
      "12: conv2d_5 — Trainable: True\n",
      "13: add_3 — Trainable: True\n",
      "14: activation_11 — Trainable: True\n",
      "15: separable_conv2d_9 — Trainable: True\n",
      "16: batch_normalization_11 — Trainable: True\n",
      "17: activation_12 — Trainable: True\n",
      "18: separable_conv2d_10 — Trainable: True\n",
      "19: batch_normalization_12 — Trainable: True\n",
      "20: max_pooling2d_4 — Trainable: True\n",
      "21: conv2d_6 — Trainable: True\n",
      "22: add_4 — Trainable: False\n",
      "23: activation_13 — Trainable: False\n",
      "24: separable_conv2d_11 — Trainable: False\n",
      "25: batch_normalization_13 — Trainable: False\n",
      "26: activation_14 — Trainable: False\n",
      "27: separable_conv2d_12 — Trainable: False\n",
      "28: batch_normalization_14 — Trainable: False\n",
      "29: max_pooling2d_5 — Trainable: False\n",
      "30: conv2d_7 — Trainable: False\n",
      "31: add_5 — Trainable: False\n",
      "32: separable_conv2d_13 — Trainable: False\n",
      "33: batch_normalization_15 — Trainable: False\n",
      "34: activation_15 — Trainable: False\n",
      "35: global_average_pooling2d_1 — Trainable: False\n",
      "36: dropout_1 — Trainable: False\n",
      "37: dense_1 — Trainable: True\n"
     ]
    }
   ],
   "source": [
    "# Ensure the new model is compiled with the goof layer unfrozen\n",
    "for i, layer in enumerate(new_model.layers):\n",
    "    print(f\"{i}: {layer.name} — Trainable: {layer.trainable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Modification for experiment 1\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23422 files belonging to 2 classes.\n",
      "Using 18738 files for training.\n",
      "Using 4684 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "image_size = (180, 180)\n",
    "batch_size = 128\n",
    "\n",
    "train_ds, val_ds = keras.utils.image_dataset_from_directory(\n",
    "    \"PetImages\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "]\n",
    "\n",
    "\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images\n",
    "\n",
    "augmented_train_ds = train_ds.map(\n",
    "    lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "# Apply `data_augmentation` to the training images.\n",
    "train_ds = train_ds.map(\n",
    "    lambda img, label: (data_augmentation(img), label),\n",
    "    num_parallel_calls=tf_data.AUTOTUNE,\n",
    ")\n",
    "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "train_ds = train_ds.prefetch(tf_data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1007s\u001b[0m 6s/step - acc: 0.5870 - loss: 0.7095 - val_acc: 0.5352 - val_loss: 0.6903\n",
      "Epoch 2/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m998s\u001b[0m 7s/step - acc: 0.6797 - loss: 0.5930 - val_acc: 0.5186 - val_loss: 0.6941\n",
      "Epoch 3/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m929s\u001b[0m 6s/step - acc: 0.7174 - loss: 0.5537 - val_acc: 0.4964 - val_loss: 0.7212\n",
      "Epoch 4/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m989s\u001b[0m 7s/step - acc: 0.7347 - loss: 0.5270 - val_acc: 0.5083 - val_loss: 0.7819\n",
      "Epoch 5/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m945s\u001b[0m 6s/step - acc: 0.7501 - loss: 0.5070 - val_acc: 0.6800 - val_loss: 0.5758\n",
      "Epoch 6/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1153s\u001b[0m 8s/step - acc: 0.7729 - loss: 0.4798 - val_acc: 0.7737 - val_loss: 0.4780\n",
      "Epoch 7/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1307s\u001b[0m 9s/step - acc: 0.7733 - loss: 0.4740 - val_acc: 0.7882 - val_loss: 0.4580\n",
      "Epoch 8/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1193s\u001b[0m 8s/step - acc: 0.7918 - loss: 0.4422 - val_acc: 0.7959 - val_loss: 0.4357\n",
      "Epoch 9/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 6s/step - acc: 0.8046 - loss: 0.4248 - val_acc: 0.8104 - val_loss: 0.4129\n",
      "Epoch 10/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m888s\u001b[0m 6s/step - acc: 0.8180 - loss: 0.4057 - val_acc: 0.8006 - val_loss: 0.4307\n",
      "Epoch 11/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 6s/step - acc: 0.8252 - loss: 0.3866 - val_acc: 0.8326 - val_loss: 0.3775\n",
      "Epoch 12/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m903s\u001b[0m 6s/step - acc: 0.8345 - loss: 0.3753 - val_acc: 0.8228 - val_loss: 0.4097\n",
      "Epoch 13/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m893s\u001b[0m 6s/step - acc: 0.8420 - loss: 0.3660 - val_acc: 0.7797 - val_loss: 0.4602\n",
      "Epoch 14/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1003s\u001b[0m 7s/step - acc: 0.8516 - loss: 0.3380 - val_acc: 0.8527 - val_loss: 0.3386\n",
      "Epoch 15/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m922s\u001b[0m 6s/step - acc: 0.8559 - loss: 0.3302 - val_acc: 0.8343 - val_loss: 0.3719\n",
      "Epoch 16/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m928s\u001b[0m 6s/step - acc: 0.8598 - loss: 0.3212 - val_acc: 0.8173 - val_loss: 0.3911\n",
      "Epoch 17/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1071s\u001b[0m 7s/step - acc: 0.8691 - loss: 0.3056 - val_acc: 0.8832 - val_loss: 0.2884\n",
      "Epoch 18/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m978s\u001b[0m 7s/step - acc: 0.8756 - loss: 0.2859 - val_acc: 0.8493 - val_loss: 0.3378\n",
      "Epoch 19/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m935s\u001b[0m 6s/step - acc: 0.8820 - loss: 0.2830 - val_acc: 0.8548 - val_loss: 0.3413\n",
      "Epoch 20/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m851s\u001b[0m 6s/step - acc: 0.8873 - loss: 0.2707 - val_acc: 0.8414 - val_loss: 0.3543\n",
      "Epoch 21/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m970s\u001b[0m 6s/step - acc: 0.8918 - loss: 0.2589 - val_acc: 0.8928 - val_loss: 0.2622\n",
      "Epoch 22/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m889s\u001b[0m 6s/step - acc: 0.8922 - loss: 0.2587 - val_acc: 0.8888 - val_loss: 0.2644\n",
      "Epoch 23/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1080s\u001b[0m 7s/step - acc: 0.8929 - loss: 0.2553 - val_acc: 0.8305 - val_loss: 0.3799\n",
      "Epoch 24/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1035s\u001b[0m 7s/step - acc: 0.9038 - loss: 0.2318 - val_acc: 0.8807 - val_loss: 0.2813\n",
      "Epoch 25/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1162s\u001b[0m 8s/step - acc: 0.9016 - loss: 0.2314 - val_acc: 0.8777 - val_loss: 0.3015\n",
      "Epoch 26/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1353s\u001b[0m 9s/step - acc: 0.9076 - loss: 0.2254 - val_acc: 0.8450 - val_loss: 0.3575\n",
      "Epoch 27/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1108s\u001b[0m 7s/step - acc: 0.9094 - loss: 0.2194 - val_acc: 0.8986 - val_loss: 0.2450\n",
      "Epoch 28/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1184s\u001b[0m 8s/step - acc: 0.9098 - loss: 0.2111 - val_acc: 0.9135 - val_loss: 0.2180\n",
      "Epoch 29/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m938s\u001b[0m 6s/step - acc: 0.9123 - loss: 0.2126 - val_acc: 0.8207 - val_loss: 0.4072\n",
      "Epoch 30/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1039s\u001b[0m 7s/step - acc: 0.9125 - loss: 0.2077 - val_acc: 0.9135 - val_loss: 0.2145\n",
      "Epoch 31/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m899s\u001b[0m 6s/step - acc: 0.9207 - loss: 0.1947 - val_acc: 0.8772 - val_loss: 0.3004\n",
      "Epoch 32/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1035s\u001b[0m 7s/step - acc: 0.9172 - loss: 0.1976 - val_acc: 0.8166 - val_loss: 0.4716\n",
      "Epoch 33/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1400s\u001b[0m 9s/step - acc: 0.9194 - loss: 0.1913 - val_acc: 0.9041 - val_loss: 0.2411\n",
      "Epoch 34/50\n",
      "\u001b[1m 92/147\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m7:39\u001b[0m 8s/step - acc: 0.9261 - loss: 0.1769"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath=\"models/experiment3_epoch_{epoch}.keras\")\n",
    "]\n",
    "\n",
    "\n",
    "new_model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "new_model.save(\"models/experiment3_model.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
