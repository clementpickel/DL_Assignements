{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4\n",
    "- Load the saved model and replace the output layer of the model, as well as the two last convolutional layers.\n",
    "- Train and evaluate the model on the cats and dogs dataset.\n",
    "\n",
    "## Steps\n",
    "- Load the saved model using keras.models.load_model().\n",
    "- Freeze all layers except the output layer and the last two convolutional layers.\n",
    "- Replace the output layer to match the number of classes (num_classes = 2 for cats vs. dogs).\n",
    "- Retrain the model on the Cats vs. Dogs dataset.\n",
    "\n",
    "### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "# Load the pre-trained model\n",
    "saved_model_path = \"models/experiment1_model.keras\"\n",
    "model = keras.models.load_model(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the model\n",
    "See which layer need to be reset, in our case 23 to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: input_layer_1\n",
      "Layer 1: rescaling_1\n",
      "Layer 2: conv2d_4\n",
      "Layer 3: batch_normalization_8\n",
      "Layer 4: activation_8\n",
      "Layer 5: activation_9\n",
      "Layer 6: separable_conv2d_7\n",
      "Layer 7: batch_normalization_9\n",
      "Layer 8: activation_10\n",
      "Layer 9: separable_conv2d_8\n",
      "Layer 10: batch_normalization_10\n",
      "Layer 11: max_pooling2d_3\n",
      "Layer 12: conv2d_5\n",
      "Layer 13: add_3\n",
      "Layer 14: activation_11\n",
      "Layer 15: separable_conv2d_9\n",
      "Layer 16: batch_normalization_11\n",
      "Layer 17: activation_12\n",
      "Layer 18: separable_conv2d_10\n",
      "Layer 19: batch_normalization_12\n",
      "Layer 20: max_pooling2d_4\n",
      "Layer 21: conv2d_6\n",
      "Layer 22: add_4\n",
      "Layer 23: activation_13\n",
      "Layer 24: separable_conv2d_11\n",
      "Layer 25: batch_normalization_13\n",
      "Layer 26: activation_14\n",
      "Layer 27: separable_conv2d_12\n",
      "Layer 28: batch_normalization_14\n",
      "Layer 29: max_pooling2d_5\n",
      "Layer 30: conv2d_7\n",
      "Layer 31: add_5\n",
      "Layer 32: separable_conv2d_13\n",
      "Layer 33: batch_normalization_15\n",
      "Layer 34: activation_15\n",
      "Layer 35: global_average_pooling2d_1\n",
      "Layer 36: dropout_1\n",
      "Layer 37: dense_1\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for layer in model.layers:\n",
    "    print(f\"Layer {i}: {layer.name}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        units = 1\n",
    "    else:\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    # Change output layer for multi-class classification (120 dog breeds)\n",
    "    outputs = layers.Dense(units, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh model with the same architecture\n",
    "fresh_model = make_model(input_shape=(180, 180) + (3,), num_classes=1)\n",
    "base_model = keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "base_model.trainable = False \n",
    "\n",
    "# Reset weights for layers 6 to 22\n",
    "for i in range(14, 31):\n",
    "    base_model.layers[i].set_weights(fresh_model.layers[i].get_weights())\n",
    "    base_model.layers[i].trainable = True  # Make sure the layers are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new output layer (Binary Classification)\n",
    "new_output = layers.Dense(1, activation=\"sigmoid\")(base_model.output)\n",
    "\n",
    "# Create a new model\n",
    "new_model = keras.Model(inputs=base_model.input, outputs=new_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: input_layer_1 — Trainable: False\n",
      "1: rescaling_1 — Trainable: False\n",
      "2: conv2d_4 — Trainable: False\n",
      "3: batch_normalization_8 — Trainable: False\n",
      "4: activation_8 — Trainable: False\n",
      "5: activation_9 — Trainable: False\n",
      "6: separable_conv2d_7 — Trainable: False\n",
      "7: batch_normalization_9 — Trainable: False\n",
      "8: activation_10 — Trainable: False\n",
      "9: separable_conv2d_8 — Trainable: False\n",
      "10: batch_normalization_10 — Trainable: False\n",
      "11: max_pooling2d_3 — Trainable: False\n",
      "12: conv2d_5 — Trainable: False\n",
      "13: add_3 — Trainable: False\n",
      "14: activation_11 — Trainable: True\n",
      "15: separable_conv2d_9 — Trainable: True\n",
      "16: batch_normalization_11 — Trainable: True\n",
      "17: activation_12 — Trainable: True\n",
      "18: separable_conv2d_10 — Trainable: True\n",
      "19: batch_normalization_12 — Trainable: True\n",
      "20: max_pooling2d_4 — Trainable: True\n",
      "21: conv2d_6 — Trainable: True\n",
      "22: add_4 — Trainable: True\n",
      "23: activation_13 — Trainable: True\n",
      "24: separable_conv2d_11 — Trainable: True\n",
      "25: batch_normalization_13 — Trainable: True\n",
      "26: activation_14 — Trainable: True\n",
      "27: separable_conv2d_12 — Trainable: True\n",
      "28: batch_normalization_14 — Trainable: True\n",
      "29: max_pooling2d_5 — Trainable: True\n",
      "30: conv2d_7 — Trainable: True\n",
      "31: add_5 — Trainable: False\n",
      "32: separable_conv2d_13 — Trainable: False\n",
      "33: batch_normalization_15 — Trainable: False\n",
      "34: activation_15 — Trainable: False\n",
      "35: global_average_pooling2d_1 — Trainable: False\n",
      "36: dropout_1 — Trainable: False\n",
      "37: dense_3 — Trainable: True\n"
     ]
    }
   ],
   "source": [
    "# Ensure the new model is compiled with the goof layer unfrozen\n",
    "for i, layer in enumerate(new_model.layers):\n",
    "    print(f\"{i}: {layer.name} — Trainable: {layer.trainable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Modification for experiment 1\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23422 files belonging to 2 classes.\n",
      "Using 18738 files for training.\n",
      "Using 4684 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "image_size = (180, 180)\n",
    "batch_size = 128\n",
    "\n",
    "train_ds, val_ds = keras.utils.image_dataset_from_directory(\n",
    "    \"PetImages\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "]\n",
    "\n",
    "\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images\n",
    "\n",
    "augmented_train_ds = train_ds.map(\n",
    "    lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "# Apply `data_augmentation` to the training images.\n",
    "train_ds = train_ds.map(\n",
    "    lambda img, label: (data_augmentation(img), label),\n",
    "    num_parallel_calls=tf_data.AUTOTUNE,\n",
    ")\n",
    "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "train_ds = train_ds.prefetch(tf_data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m928s\u001b[0m 5s/step - acc: 0.6208 - loss: 0.6621 - val_acc: 0.4966 - val_loss: 0.7056\n",
      "Epoch 2/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 4s/step - acc: 0.7243 - loss: 0.5447 - val_acc: 0.4979 - val_loss: 0.7116\n",
      "Epoch 3/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 4s/step - acc: 0.7513 - loss: 0.5105 - val_acc: 0.5096 - val_loss: 0.7054\n",
      "Epoch 4/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 4s/step - acc: 0.7797 - loss: 0.4654 - val_acc: 0.5807 - val_loss: 0.6693\n",
      "Epoch 5/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.8020 - loss: 0.4300 - val_acc: 0.6552 - val_loss: 0.6029\n",
      "Epoch 6/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.8212 - loss: 0.3967 - val_acc: 0.8081 - val_loss: 0.4239\n",
      "Epoch 7/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.8329 - loss: 0.3743 - val_acc: 0.8456 - val_loss: 0.3604\n",
      "Epoch 8/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.8485 - loss: 0.3439 - val_acc: 0.8427 - val_loss: 0.3593\n",
      "Epoch 9/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.8575 - loss: 0.3211 - val_acc: 0.8064 - val_loss: 0.4161\n",
      "Epoch 10/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.8660 - loss: 0.3115 - val_acc: 0.8427 - val_loss: 0.3558\n",
      "Epoch 11/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.8724 - loss: 0.2946 - val_acc: 0.8546 - val_loss: 0.3262\n",
      "Epoch 12/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.8786 - loss: 0.2828 - val_acc: 0.8155 - val_loss: 0.4121\n",
      "Epoch 13/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m554s\u001b[0m 4s/step - acc: 0.8800 - loss: 0.2747 - val_acc: 0.8597 - val_loss: 0.3233\n",
      "Epoch 14/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m554s\u001b[0m 4s/step - acc: 0.8926 - loss: 0.2619 - val_acc: 0.8813 - val_loss: 0.2836\n",
      "Epoch 15/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 4s/step - acc: 0.9001 - loss: 0.2404 - val_acc: 0.8702 - val_loss: 0.3007\n",
      "Epoch 16/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 4s/step - acc: 0.8991 - loss: 0.2391 - val_acc: 0.8698 - val_loss: 0.3158\n",
      "Epoch 17/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 4s/step - acc: 0.9030 - loss: 0.2361 - val_acc: 0.8986 - val_loss: 0.2513\n",
      "Epoch 18/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.9036 - loss: 0.2254 - val_acc: 0.8941 - val_loss: 0.2493\n",
      "Epoch 19/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9090 - loss: 0.2190 - val_acc: 0.8121 - val_loss: 0.4092\n",
      "Epoch 20/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.9128 - loss: 0.2039 - val_acc: 0.9024 - val_loss: 0.2442\n",
      "Epoch 21/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9164 - loss: 0.2013 - val_acc: 0.8390 - val_loss: 0.3849\n",
      "Epoch 22/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 4s/step - acc: 0.9217 - loss: 0.1924 - val_acc: 0.9020 - val_loss: 0.2327\n",
      "Epoch 23/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9247 - loss: 0.1848 - val_acc: 0.9031 - val_loss: 0.2312\n",
      "Epoch 24/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9286 - loss: 0.1758 - val_acc: 0.9069 - val_loss: 0.2365\n",
      "Epoch 25/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9297 - loss: 0.1739 - val_acc: 0.9037 - val_loss: 0.2355\n",
      "Epoch 26/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9313 - loss: 0.1702 - val_acc: 0.8997 - val_loss: 0.2405\n",
      "Epoch 27/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 4s/step - acc: 0.9323 - loss: 0.1671 - val_acc: 0.9129 - val_loss: 0.2189\n",
      "Epoch 28/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 4s/step - acc: 0.9372 - loss: 0.1594 - val_acc: 0.8523 - val_loss: 0.3407\n",
      "Epoch 29/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 4s/step - acc: 0.9390 - loss: 0.1548 - val_acc: 0.8474 - val_loss: 0.4178\n",
      "Epoch 30/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.9348 - loss: 0.1602 - val_acc: 0.8988 - val_loss: 0.2365\n",
      "Epoch 31/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.9424 - loss: 0.1469 - val_acc: 0.9197 - val_loss: 0.2044\n",
      "Epoch 32/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.9431 - loss: 0.1435 - val_acc: 0.9082 - val_loss: 0.2406\n",
      "Epoch 33/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.9403 - loss: 0.1459 - val_acc: 0.9129 - val_loss: 0.2185\n",
      "Epoch 34/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9429 - loss: 0.1380 - val_acc: 0.8990 - val_loss: 0.2468\n",
      "Epoch 35/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9457 - loss: 0.1356 - val_acc: 0.9159 - val_loss: 0.2102\n",
      "Epoch 36/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.9479 - loss: 0.1289 - val_acc: 0.9210 - val_loss: 0.2001\n",
      "Epoch 37/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9473 - loss: 0.1355 - val_acc: 0.9152 - val_loss: 0.2004\n",
      "Epoch 38/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9462 - loss: 0.1313 - val_acc: 0.8975 - val_loss: 0.2689\n",
      "Epoch 39/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9531 - loss: 0.1212 - val_acc: 0.9101 - val_loss: 0.2316\n",
      "Epoch 40/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9540 - loss: 0.1166 - val_acc: 0.8354 - val_loss: 0.4909\n",
      "Epoch 41/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9538 - loss: 0.1155 - val_acc: 0.8623 - val_loss: 0.4082\n",
      "Epoch 42/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 4s/step - acc: 0.9552 - loss: 0.1145 - val_acc: 0.8749 - val_loss: 0.3170\n",
      "Epoch 43/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9551 - loss: 0.1094 - val_acc: 0.9180 - val_loss: 0.2274\n",
      "Epoch 44/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9588 - loss: 0.1081 - val_acc: 0.9135 - val_loss: 0.2318\n",
      "Epoch 45/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9573 - loss: 0.1064 - val_acc: 0.8950 - val_loss: 0.2942\n",
      "Epoch 46/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9586 - loss: 0.1043 - val_acc: 0.9197 - val_loss: 0.2062\n",
      "Epoch 47/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9578 - loss: 0.1071 - val_acc: 0.9276 - val_loss: 0.1819\n",
      "Epoch 48/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9640 - loss: 0.0926 - val_acc: 0.8096 - val_loss: 0.5698\n",
      "Epoch 49/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9614 - loss: 0.0984 - val_acc: 0.9033 - val_loss: 0.2731\n",
      "Epoch 50/50\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 4s/step - acc: 0.9642 - loss: 0.0915 - val_acc: 0.9253 - val_loss: 0.1965\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath=\"models/experiment4_epoch_{epoch}.keras\")\n",
    "]\n",
    "\n",
    "new_model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "new_model.save(\"experiment4_model.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
